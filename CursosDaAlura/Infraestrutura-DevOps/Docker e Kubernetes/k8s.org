* Toda vez que quiser iniciar o cluster no Linux
rodar antes:

	minikube start --vm-driver=virtualbox

	minikube start  (AP: esse não tenho certeza se já não rodou acima)
* Comandos gerais
kubectl get pods -o wide

kubectl delete pod portal-noticias

kubectl apply -f portal-noticias.yaml

kubectl describe pod pod-2

kubectl get service

kubectl exec -it pod-1 -- bash

kubectl get nodes -o wide 

kubectl delete pod --all

kubectl delete svc --all

kubectl get configmap

kubectl describe configmap db-configmap

kubectl get rs (ou kubectl get replicasets) - traz tanto os replicasets como os deployments

kubectl delete -f deletaPassandoUmNomeDeArquivoDeQualquerTipo.yaml
----------
kubectl get deployments

kubectl rollout history deployment nginx-deployment 

kubectl apply-f nginx-deployment.yaml --record

kubectl rollout undo deployment
(ou: kubectl rollout undo deployment --to-revision=2 )

kubectl delete deployment nginx-deployment 

kubectl get hpa
----------
 kubectl port-forward --address 0.0.0.0 service/svc-sistema-noticias 30001:80
direciona todas as requisições da porta 30001 dessa máquina local para a porta 80 do service/svc-sistema-noticias
fazer o teste de subir nessa ordem o sistema: configmap's, pod's, svc's


AP: Quando o professor do curso atualizava um .yaml ele parava o mesmo (kubectl delete) e subi o mesmo novamente. (Em visão de considerar que apesar rodar o kubectl apply já atualiza).

Quando roda kubect get svc, o campo "NAME" é o DNS do serviço.
* Curso de Kubernetes: Pods, Services e ConfigMaps]
https://cursos.alura.com.br/course/kubernetes-pods-services-configmap
** Notas Gerais
*** Há 2 propostas de implementação de Kubernets
 - Docker Swarm
 - Kubernetes
*** Parando os pod e svc
kubectl delete pod --all

kubectl delete svc --all

** Conhecendo o Kubernets
*** O que é o Kubernets
 O Kubernetes entra do seguinte modo: eu falei para vocês agora que nós resolvemos o problema na escalabilidade horizontal dividindo o poder computacional das máquinas trabalhando em paralelo. Então o Kubernetes é capaz de fazer isso, ele gerencia uma ou múltiplas máquinas trabalhando em conjunto, que nós vamos chamar de cluster.

Uma ou mais máquinas trabalhando em conjunto, dividindo o seu poder computacional, nós vamos chamar de um *cluster*. O Kubernetes é capaz de criar esse cluster e o gerenciar para nós.

É aí que Kubernetes entra na história! Então nós conseguimos encontrar um cluster com Kubernetes; seja na AWS, seja no Google Cloud Plataform e na Azure também, aqui com Minikube no final.

O Kubernetes é capaz de criar e gerenciar um cluster para que nós consigamos manter a nossa aplicação escalável sempre que nós quisermos adicionar novos containers, sempre que nós quisermos reiniciar a nossa aplicação de maneira automática, caso ela tenha falhado. Então nós chamamos isso de orquestração de containers.

*** Arquitetura do Kubernets
ver as imagens: "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula1_video3_imagem1.png" a "aula1_video3_imagem4"
** Criando o cluster
*** Inicializando o cluster no Windows
precisamos intermediar pelo Docker
*** Inicializando o cluster no Linux
Pro/AP: Tanto o Kubernets do Windows como o do Google Cloud Platform usam Linux debaixo dos panos.
-----------Para instalar
ir em https://kubernetes.io/releases/download/

   curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

-----------

[00:29] O segundo passo agora é para tornar o Kubectl que nós estamos baixando agora para nós darmos permissão de executável para ele no nosso sistema. Então, copiando e colando. E por fim, nós movemos ele para o nosso path sem nenhum problema, mais uma vez nós colocamos a nossa senha e sem problemas.

[00:47] Para confirmar se tudo foi instalado sem nenhum problema, nós executamos esse comando. E repare que ele executou e nos retornou as informações do Kubectl.

	kubectl version --client


[00:55] Se nós executarmos aquele mesmo comando que nós fizemos no Windows do Kubectl get nodes, o que vai acontecer? Repare que ele deu um erro de conexão recusada, porque nós não temos um cluster ainda. Sem cluster nós não temos API, logo nós não estamos nos comunicando com ninguém.

	kubectl get nodes

[01:11] E para nós termos o nosso cluster, a nossa API em si, nós vamos utilizar uma ferramenta chamada Minikube, onde ela já cria um ambiente virtualizado com o cluster pronto para nós.

	curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
	sudo install minikube-linux-amd64 /usr/local/bin/minikube


	sudo mkdir -p /usr/local/bin 
	sudo install minikube /usr/local/bin/


Ir em: https://www.virtualbox.org/wiki/Linux_Downloads
Ele será usado como driver de virtualização:
e baixar o arquivo semelhante à: virtualbox-7.0_7.0.14-161095~Ubuntu~jammy_amd64
(essa era a última versão quando acessei)

dpkg -i virtualbox-7.0_7.0.14-161095~Ubuntu~jammy_amd64

(Obs: eu precisei instalar além do que o professor do curso apresentou também: sudo apt-get install virtualbox-dkms)

minikube start --vm-driver=virtualbox

[01:53] Se nós executarmos Minikube, nós veremos que apareceram diversas opções. O mais importante é a opção do minikube start, onde ele vai criar para nós um cluster local do Kubernetes na nossa máquina virtualizada.

	minikube start

[02:13] E para nós executarmos esse comando do minikube start, nós precisamos informar para ele mais uma coisa: qual é o drive de virtualização que nós vamos utilizar para criar esse cluster? AP: Foi o passado no argumento: virtualbox

[03:14] Nós não vamos utilizar o VirtualBox fisicamente. Nós não vamos lidar com ele diretamente, nós só vamos utilizar essa ferramenta como o nosso driver de virtualização.

*onde nós estamos falando que o Minikube, que ele vai utilizar o VirtualBox como driver de virtualização para criar um ambiente virtualizado com o nosso cluster kubernetes dentro. E o melhor: o Kubectl já vai conseguir fazer essa comunicação de maneira automática.*

[04:13] Repare que ele terminou e no final ele ainda nos mostra que o Kubectl já está até configurado para usar o Minikube.

[04:21] Então se agora nós executarmos o nosso comando 
	
	kubectl get nodes

repare o que vai acontecer: ele nos exibe o nosso nó chamado Minikube com status de Ready e o papel aqui de master, sem nenhum problema.

[04:35] Mas caso você que está acompanhando essa aula e vai fazer todo o curso no Linux, a única diferença que você vai ter em relação até então ao Windows, é que sempre que você iniciar a sua máquina:

	minikube start --vm-driver =virtualbox

[04:57] No Linux, sempre que você iniciar o seu sistema e você for fazer algo relativo ao curso, você vai precisar executar esse comando minikube start --vm-driver=virtualbox novamente, que ele vai reiniciar a sua máquina virtual e o seu cluster consequentemente, para que você consiga se comunicar efetivamente com o seu cluster, ele vai precisar estar iniciado.

** Criando e entendendo pods
*** Entendendo o que são pods
(Obs: ver imagens: "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula3-video1_imagem1.png" a "aula3-video1_imagem5.png")

[00:00] Agora nós vamos entender o que é esse termo tão famoso quando nós ouvimos falar de Kubernetes, que são os pods. Nós vamos entender do que se trata, qual a diferença dele para um container, qual a vantagem da utilização de um pod, porque nós devemos utilizar ele e em qual cenário nós devemos utilizar.

[00:16] Então vamos lá! Nós podemos começar fazendo aqui uma analogia com um Docker. Nós sabemos que o mundo Docker nós criamos, produzimos, gerenciamos e manipulamos o nosso container; não é verdade?

[00:28] Então no mundo Docker nós trabalhamos com container. E a partir de agora no Kubernetes nós vamos criar, produzir, manipular e gerenciar - não mais os containers diretamente, e sim os nossos pods. Então o mundo kubernetes, pods, o mundo Docker e containers.

[00:47] Então está aí uma diferença já de cara que nós vamos começar trabalhar agora com os pods. Mas o que é um pod? Vamos entender agora. Um pod, se nós traduzirmos literalmente, ele é uma capsula na verdade, e uma capsula pode conter um ou mais containers dentro dela.

[01:06] Então nós entendemos já a diferença para um pod e entre um pod e um container. Nós sabemos que um pod é um conjunto de um ou mais containers, mas o que isso muda na pratica?

[01:17] A partir de agora então, quando nós tivermos aqui a comunicação da nossa máquina com o kubectl para API, nós não vamos pedir pela criação diretamente de um container, e sim de um pod, que pode conter um ou mais containers dentro dele.

[01:32] Isso sempre de maneira declarativa ou imperativa. 

[01:40] Dentro de *um pod* nós temos liberdade, como eu falei para vocês de termos mais containers, mas sempre que nós criamos um pod ele ganha um endereço IP. (AP: *dentro de um pod podemos ter 1 ou mais containers*).

[01:49] Então o endereço IP não é mais do container, e sim do nosso pod. Dentro do nosso pod nós temos total liberdade de fazermos um mapeamento de portas para os IPs que são atribuídos a esse pod. Então, o que isso quer dizer? Vamos entender agora!

[02:06] No momento em que nós fazemos a requisição aqui, por exemplo, para o IP 10.0.0.1, repare que é o mesmo IP que nós estamos fazendo requisição para o IP do pod na porta 8080. Nós estamos nos referindo nesse momento ao nosso container dentro da porta :8080 no nosso pod.

[02:25] A mesma coisa se nós tivermos outro container na porta 9000. Quando nós fizermos a requisição para esta porta neste endereço, nós vamos estar nos referindo a esse container :9000.

[02:36] O que isso quer dizer? Quer dizer que eles estão compartilhando o mesmo endereço IP e nós consequentemente não podemos ter dois containers na mesma porta dentro de um mesmo pod.

[02:48] Seguindo então, o que mais os pods são capazes de fazer? Nós vimos que nós temos um container ou mais dentro de um pod. Caso esse container falhe, o que vai acontecer? 

[03:02] (AP: Peguemos o caso de um pod ter apenas um container)Nesse momento, esse pod vai parar de funcionar. Ele morreu para sempre e o kubernetes tem total liberdade de criar um novo pod para substituir o antigo, mas não necessariamente com o mesmo IP que ele tinha antes, nós não temos controle sobre isso.

[03:19] Por quê? Porquê *os pods são efêmeros*, eles estão ali para serem substituídos a qualquer momento e toda criação de um novo pod é um novo pod efetivamente, não é o mesmo pod antigo que foi renascido.

[03:36] E caso nós tivéssemos mais de um container dentro do mesmo pod, o que iria acontecer se esse pod falhasse? Para ele falhar efetivamente nós teríamos que ter a seguinte condição:

[03:44] O primeiro container falhou dentro de um pod. *Caso ainda tenha algum container em funcionamento sem nenhum problema dentro desse mesmo pod, ele ainda está saudável*; mas caso nenhum container mais esteja funcionando dentro desse pod, esse pod foi finalizado e outro vai ser criado no lugar dele.

[04:06] Por fim, vamos entender outra questão aqui de rede do nossos pods. Agora, como mostrei para vocês, nós vamos fazer esse mapeamento de portas entre o IP do pod e aqui os nossos containers, porque agora todo IP pertence ao pod, e não aos containers.

[04:23] Isso quer dizer que no fim das contas, eles vão compartilhar os mesmos namespaces de rede e de processo, de comunicação entre o processo e eles também podem compartilhar volume. Nós vamos ver isso no decorrer do curso.

[04:35] Mas qual é a grande vantagem? Talvez você já tenha se perguntado isso na sua cabeça. Qual é a grande vantagem deles compartilharem o mesmo IP? A grande vantagem é que agora eles podem fazer essa comunicação diretamente entre eles via localhost, porque eles têm o mesmo IP, não é verdade? Que é 10.0.0.1 nesse caso.

[04:57] Então, agora nós temos essa capacidade de fazer uma comunicação de maneira muito mais fácil entre containers de um mesmo pod e isso, é claro, nós também vamos ter total capacidade de comunicar pods entre diferentes IPs. Eu tenho um pod com IP 10.0.0.1, ele pode começar com pod de IP 10.0.0.2. Por exemplo: aqui nós temos total liberdade de fazer essa comunicação.

*** O primeiro pod
Nós vamos criar o nosso primeiro pod.

[00:16] E para nós criarmos eu falei para vocês que o Kubernetes, o kubectl, é capaz de fazer operações de criar, ler, atualizar e remover os recursos de dentro do nosso cluster, se comunicando com a API.

[00:28] O comando "kubectl run" é capaz de criar um pod para nós. Os parâmetros que nós vamos informar são bem simples: o primeiro vai ser o nome do pod que nós queremos criar.

[00:41] Então eu vou criar um pod utilizando a imagem do nginx, então eu vou chamar ele de "nginx-pod" e a partir daí eu posso e devo explicitar qual imagem eu quero utilizar para basear o container que será criado dentro desse pod. Então uso a flag --image e informo com = que eu quero utilizar o nginx, por exemplo na versão latest. Então 

	kubectl run nginx-pod --image=nginx:latest

[01:04] Se eu apertar a tecla “Enter”, olhe o que vai acontecer: ele falou que criou. Será que criou? Vamos ver aqui com o comando 

	kubectl get pods

Está aqui o nosso pod chamado nginx-pod, ainda não está pronto e está com status de criação.

[01:19] Se nós executarmos esse mesmo comando 

	kubectl get pods --watch

 ele vai passar a acompanhar esse comando em tempo real. Então assim que tiver uma mudança no status desse comando, ele vai nos atualizar. Isso significa que assim que o nosso pod for criado, como ele acabou de ser, ele nos atualiza automaticamente.

[01:40] Então nós podemos apertar as teclas “Ctrl + C” para sairmos desse comando e o nosso pod já está em execução, nós podemos ver outras informações também sobre ele, com o comando

	kubectl describe pod nomeDoNossoPod
No nosso caso: 
	kubectl describe pod nginx-pod

 E eu quero descrever esse meu pod chamado nginx-pod. Nós apertamos a tecla “Enter” e ele vai exibir diversas informações. (AP: abaixo a saida do meu terminal:)

Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  2m15s  default-scheduler  Successfully assigned default/nginx-pod to minikube
  Normal  Pulling    2m15s  kubelet            Pulling image "nginx:latest"
  Normal  Pulled     2m4s   kubelet            Successfully pulled image "nginx:latest" in 10.896s (10.896s including waiting)
  Normal  Created    2m4s   kubelet            Created container nginx-pod
  Normal  Started    2m4s   kubelet            Started container nginx-pod


[02:00] Inclusive, no final nós conseguimos ver como foi o processo de criação desse pod. Primeiro ele atribuiu este pod a um nó chamado Docker Desktop, no caso do Linux vai instalar o Minikube e quem fez isso foi o “Scheduled”. Olhe que legal! Como é importante nós sabermos essa questão arquitetural do Kubernetes!

[02:19] A partir daí ele começou a fazer o download da imagem. Baixou ela com sucesso, criou o container e iniciou o pod. Então repare: o pod só foi iniciado depois da criação do container que vai compor esse pod.

[02:34] Nós podemos também ter outras informações, como por exemplo: o IP dele, esses labels e essas etiquetas que nós vamos entender do que que se tratam, pois elas são bem importantes e poderosas. Nós vamos entender bastante sobre elas no decorrer do curso, além de o nome dele e informações bem básicas sobre o nosso pod.

[02:53] Se, digamos, eu estou usando a versão nginx:latest, digamos que eu queira mudar a versão do nginx que estou utilizando nesse pod. Eu quero atualizar esse pod já existente.

[03:05] Eu tenho o comando 

	kubectl edit pod nameDoPod
no nosso caso:
	kubectl edit pod nginx-pod

e eu posso editar o quê? Um pod e qual é o pod que eu quero editar? Esse chamado nginx-pod, e ele vai abrir esse bloco de notas na nossa frente com diversas informações bem complexas. AP: Obs: no caso do Linux ele abre o vi.

[03:21] Mas o que importa para nós? Nós vamos aceitar isso por enquanto, porque nós estamos trabalhando de maneira bem ingênua. Nós queremos atualizar a imagem do nosso pod, que se nós analisarmos bem, está logo embaixo com o nosso image. Nós não queremos utilizar a versão latest, nós queremos utilizar a versão 1.0.
(editando a linha de " image: nginx:latest" para " image: nginx:1.0")

[03:43] Nós salvamos o arquivo, fechamos e ele vai falar que o nosso pod foi editado. Se nós vermos aqui de novo o nosso comando kubectl get pods, olha o que vai acontecer: ele está agora com status de 0/1, de Ready, e deu erro de imagem para baixar.

[04:01] O que isso quer dizer? Vamos descobrir o que isso quer dizer utilizando aqui o nosso comando kubectl describe pod e vamos passar aqui o nosso nginx-pod.

[04:10] Se nós vermos aqui em baixo sem nenhum problema, olhe o que aconteceu - ele começou a tentar baixar essa imagem da versão 1.0 do nginx e não conseguiu. Por quê? Porque essa imagem não existe, então ele caiu meio que em um looping, no fim das contas de ficar tentando baixar essa imagem e não conseguir.

[04:29] Por isso que se nós viermos aqui agora de novo, no status, nós estamos com esse ImagePullBackOff, porque ele não conseguiu fazer o download dessa imagem para a criação do nosso pod.

[04:40] E foi um pouco complexo porque nós fizemos isso de maneira ingênua, *nós criamos esse pod de maneira imperativa e nós tentamos editar ele também de maneira imperativa. Nós fizemos essa edição, na verdade, de maneira imperativa.*

[04:55] *Só que, qual é o problema da maneira imperativa? Nós acabamos não tendo meio que o acompanhamento de como tudo está acontecendo dentro do nosso cluster, nós não temos nada muito bem declarado e definido. Nós precisamos ter um histórico de quais comandos nós realizamos para saber qual é o nosso estado atual.*

[05:11] Para evitarmos esse tipo de problema e deixarmos tudo muito mais claro e organizado no nosso cluster, nós vamos passar a trabalhar com maneira declarativa, usando um arquivo de definição para definir como é o pod que nós queremos criar.
*** Para saber mais: Onde as imagens são armazenadas
Executamos o nosso primeiro Pod. Porém, como o Kubernetes armazena as imagens baixadas dentro do cluster?

A resposta é simples: quando definimos que um Pod será executado, o scheduler definirá em qual Node isso acontecerá. O resultado então é que as imagens quando baixadas de repositórios como o Docker Hub, serão armazenadas localmente em cada Node, não sendo compartilhada por padrão entre todos os membros do cluster.
*** Criando pods de maneira declarativa
AP: Adianto aqui o yaml escrito nessa aula (arquivo "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/aula3/primeiro-pod.yaml")
apiVersion: v1
kind: Pod
metadata:
  name: primeiro-pod-declarativo       #pode-se dar qualquer nome aqui
spec:
  containers:
    - name: container-pod-1            #pode-se dar qualquer nome aqui
      image: nginx:latest


[00:00] Agora nós vamos criar o nosso primeiro pod de maneira declarativa. O que isso quer dizer? Quer dizer que agora nós vamos precisar 
trabalhar com algum editor de texto. 

[00:17] Então eu criei uma pasta e vou abrir ela, chamada “kubernetes-alura”, e dentro dela vai ser onde nós vamos fazer todo o nosso processo de criação de arquivos. Então dentro dessa pastinha nós vamos criar os nossos arquivos de definição.

[00:35] Mas como isso funciona? É bem simples na verdade, basta nós criarmos um novo arquivo dentro dessa pasta e nomear ele. Então eu vou chamar ele de “primeiro-pod” e ele precisa ter uma extensão específica para que o kubectl consiga enviar ele e a API consiga interpretar. Então, ou ele pode ser um .json, ou ele pode ser um .yaml também.

[00:57] O mais comum e fácil de se trabalhar é o .yaml, então vai ser ele que nós vamos utilizar daqui para o final do curso.

[01:04] Então dentro desses arquivos nós precisamos começar a escrever e a informar algumas coisas, como por exemplo: qual é a versão da API que nós queremos utilizar.

[01:14] “Como assim versão da API?” Se nós virmos na documentação, nós vamos entender que na verdade a API era uma única aplicação centralizada que foi dividida em diversas partes. Embaixo nós temos uma delas, por exemplo: a versão alfa, a versão beta e a versão estável.

[01:37] Onde a alfa tem coisas que podem ainda estar contendo bug; embaixo nós temos a beta que já pode ser considerada segura, mas ainda não é bom utilizar definitivamente; e a versão estável que é um “v” seguido de um número inteiro, onde é a versão estável efetivamente para uso.

[01:56] E ela possui também diversos grupos para nós utilizarmos. Como nós queremos criar um pod, o pod está dentro da versão estável da API, logo está na versão “v” seguida de algum número - nesse caso ele está na versão “v1”.  (daí: "apiVersion: v1")

[02:12] Logo depois nós precisamos informar o que nós queremos criar. Nós queremos criar um pod, então o tipo do que nós queremos criar, dos recursos que nós queremos criar, é um pod. (daí: "kind: Pod")

[02:22] Logo depois nós definimos quais são os metadados desse pod. Como, por exemplo: nós vamos definir qual nome nós vamos dar para ele, no caso dentro de metadados nós vamos definir essas informações.

[02:37] Como nós queremos fazer isso dentro de metadata, eu vou escrever que o nome que eu quero dar para esse pod vai ser o nosso "primeiro-pod-declarativo" e fechar. Não tem mais nada para colocar no meu metadado.

[02:55] E agora, quais são as especificações que eu quero dar para esse pod. Eu quero que ele contenha um container, um ou mais containers. Aqui no caso que tenho o nome de, no caso, "nginx-container", que eu posso dar qualquer nome a esse container. É irrelevante para o nosso caso. Logo depois eu posso definir qual imagem eu quero utilizar para esse container.

[03:26] Então nós queremos utilizar mais uma vez a versão do nginx na versão latest. Repare que eu coloquei um tracinho. Por quê? Eu posso ter diversos desses pares para definir exatamente essa questão, eu posso ter múltiplos containers dentro de um pod. Então esse tracinho é para marcar o início de uma nova declaração dentro do nosso container, mas nós só queremos um container dentro desse pod. Então ele está feito.

[03:54] E agora, como nós utilizamos esse arquivo de definição? É bem fácil! Pedir para o kubectl fazer o quê? Não para ele criar um pod da maneira como nós fizemos antes, mas para ele aplicar o nosso arquivo de definição chamado de primeiro-pod.yaml

	kubectl apply -f primeiro-pod.yaml 
	
[04:16] E olhe que legal, ele fala que o nosso primeiro-pod agora foi criado. Se nós dermos o comando 

	kubectl get pods 

 está ele, o nosso primeiro pod declarativo, 1/1 rodando.

[04:29] E olhe que legal - agora nós só precisamos utilizar o nosso arquivo de definição e o comando foi para entregar esse arquivo para a API fazer e tomar a ação necessária!

[04:41] Então nós não precisamos mais nos preocupar com qual comando nós vamos utilizar, e sim em entregar um arquivo de definição para o Kubernetes fazer o que nós queremos.

[04:49] Então nós vamos ficar aplicando esses arquivos de definição, declarativos para criar os nossos recursos. Olhe que legal!

[04:56] E com isso fica bem mais fácil nós manusearmos os nossos recursos. Por quê? Porque digamos que agora eu quero utilizar de novo a versão 1.0 que não existe do nginx. Basta eu vir no meu arquivo de definição, trocar para a versão 1.0 e aplicar esse arquivo novamente, o mesmo comando, a mesma ideia.

	kubectl apply -f primeiro-pod.yaml 
AP: antes trocar a linha da imagem para:
      image: nginx:1.0

[05:18] Ele vai nos informar que o pod não foi criado, e sim configurado (pod/primeiro-pod-declarativo configured); porque ele já existe e uma ação foi realizada sobre ele. Se nós formos olhar exatamente a mesma coisa da aula anterior, ele não conseguiu baixar a imagem. Se nós continuarmos repetindo isso, em algum momento ele vai cair nesse ImagePullBackOff.

[05:39] E agora nós editamos. Conseguimos editar ele de uma maneira bem mais prática em relação àquele arquivo gigante que nós tínhamos, que também era um .yaml, mas era bem mais complexo de se entender.

[05:50] Agora nós temos um arquivo mais simples, isso significa que se eu voltar e tentar colocar uma outra versão - por exemplo, a stable do nosso nginx, que é uma versão que existe; se eu voltar e aplicar de novo o nosso arquivo de definição, olhe que legal!

[06:08] Vamos executar o "kubectl get pods" e vamos observar o que vai acontecer. Ele vai continuar com esse status de erro, mas ainda ele não se configurou, ele ainda não atualizou ali efetivamente. E agora sim ele baixou e está utilizando a nova imagem.

[06:25] Se nós apertarmos as teclas “Control + C” e descrever esse nosso pod que nós fizemos o nosso primeiro pod declarativo.

[06:36] A atribuição do scheduler como antes, a criação; o erro do ImagePullBackOff, que ele continuou tentando utilizar da versão 1.0; depois a nova tentativa de baixar a versão estável e a criação. Tudo feito sem nenhum problema, olhe que legal!

[06:53] E isso tudo só com um comando,
	kubectl apply -f primeiro-pod.yaml 
 então nós centralizamos diversas dessas ações através desse único comando kubectl apply, ou seja, o kubectl foi responsável por fazer a comunicação com a API. Nós aplicamos um arquivo, esse -f de file - na verdade chamado primeiro-pod.yaml - e a mágica foi feita sem nenhum mistério, nós só definimos o que nós queríamos e isso foi criado dentro do nosso cluster.

[07:23] Então a partir de agora, o que nós estamos conseguindo fazer? Nós estamos conseguindo criar, gerenciar e manipular recursos através de um único comando de uma maneira que é bem mais usada em produção e tendo um registro de como está o nosso estado atual.

[07:39] Basta nós consultarmos um arquivo e vermos como nós queremos que o nosso recurso esteja, e ele vai estar conforme o arquivo de declaração de definição.

[07:49] No próximo vídeo nós vamos começar a colocar a mão na massa com um projeto com um pouco mais bem elaborado, que nós vamos utilizar no decorrer da parte 1 e da parte 2 desse curso, para nós conseguirmos sedimentar bem os conceitos que nós vamos aprender. 
*** Iniciando o projeto
AP: ver:  (arquivo "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/aula3/portal-noticias.yaml")

[00:00] Agora nós vamos começar a colocar a mão na massa em um projeto mais bem elaborado, para nós conseguirmos, como eu falei, sedimentar os conceitos que nós viemos aprendendo.

[00:08] Então, de início nós temos aqueles dois pods da aula passada funcionando ainda. Nós temos duas maneiras de fazer esses pods pararem de funcionar.

[00:19] *Esse que foi criado de maneira imperativa, nós só temos essa possibilidade de executarmos o comando kubectl delete pod e passamos o nome do pod que nós queremos deletar.*

	kubectl delete pod nginx-pod

[00:28] Então a partir desse momento que nós executarmos o comando kubectl get pods de novo, que está terminando de deletar, nós vamos ver que esse nginx-pod foi removido; nós não temos esse pod em execução, só o nosso primeiro-pod-declarativo, que foi criado de maneira declarativa.

[00:45] *A outra maneira que nós temos de eliminarmos um pod que foi criado também de maneira declarativa, que no caso é o nosso pod, é da seguinte maneira: nós podemos utilizar o*

	kubectl delete -f primeiro-pod.yaml   (estando no terminal no diretório do arquivo)

para passar um arquivo. Qual é o pod que nós queremos criar? O pod que está utilizando o arquivo de definição baseado no .\primeiro-pod.yaml.

[01:10] Então, ele vai bater esse nome: primeiro-pod-declarativo e vai remover esse pod. Nós apertamos a tecla “Enter” e ele também vai ser deletado. Olhe que legal!

[01:24] Então nós temos essa maneira de removermos imperativamente, mas também nós podemos remover ele em cima do nosso arquivo de definição. Olhe que legal!

[01:33] Mas vamos criar o nosso projeto! Nós vamos trabalhar em cima de um portal de notícias, só que seguindo todas as boas práticas do Kubernetes e como nós podemos utilizar os recursos ao nosso favor.

[01:44] Então, como nós vamos criar de início um pod para esse portal de notícias, que é uma imagem Docker que já existe, nós vamos criar esse pod. Vamos chamar ele de "portal-noticias.yaml".

[01:58] E dentro dele nós temos aquelas informações que nós já vimos, da versão da API. Como é um pod que está na versão V1 e o tipo que nós queremos criar, nós já sabemos que é um pod.

[02:09] Os metadados daqui que nós vamos definir, nós vimos que o nome que nós vamos definir é também arbitrário. Nós podemos colocar name: "portal-noticias", sem nenhum problema. Nós podemos dar o nome que nós quisermos, mas é sempre bom sermos semântico.

[02:24] E as especificações desse name: portal-noticias, quais são as informações do container que vai compor esse pod para nós? Ele vai ter um nome que nós temos total liberdade para definirmos. Como, por exemplo: "portal-noticias-container". Nós podemos dar o nome que nós quisermos para esse campo desse nosso container.

[02:45] E a imagem que nós vamos utilizar é uma imagem que já existe e está nesse repositório da Alura – "image: aluracursos/portal-noticias:1" (na versão 1). Nós salvamos esse arquivo e partindo daí basta nós repetirmos o nosso comando e aplicarmos o nosso arquivo de definição, passando 

	kubectl apply -f portal-noticias.yaml 

[03:10] Se agora nós escrevermos o nosso kubectl get pods –watch, ele vai começar a acompanhar esse status de criação.

[03:28] Criado, rodando sem nenhum problema. Como nós acessamos agora essa aplicação dentro desse pod que nós acabamos de criar? Nós podemos de início verificarmos qual é o IP dele com o comando

	kubectl describe pod portal-noticias

Ele vai nos exibir todo o status de que tudo está rodando sem nenhum problema. Se nós vemos o nosso IPem cima, ele é 10.1.0.9.

[03:54] Então vamos copiar. Nós podemos abrir o nosso navegador. Vamos abrir ele sem nenhum problema, vamos abrir e vamos tentar executar esse IP.

[04:08] O que vai acontecer? Pelo tempo que está demorando nós já conseguimos ter uma breve noção de que alguma coisa está errada. Então ele vai continuar tentando acessar e enquanto ele tenta acessar nós vamos tentar acessar ele de uma outra maneira.

[04:34] Nós conseguimos executar comandos dentro do nosso pod. Assim como no Docker, nós temos aquele comando docker exec. Aqui no Kubernetes, nós temos o comando kubectl exec e também de maneira interativa.

[04:47] E qual é o comando? Qual é o pod que nós queremos executar de maneira interativa? Exatamente o nosso portal-noticias. E qual comando nós queremos executar dentro dele? Nós queremos executar o comando do bash, que é o terminal ali, no caso.

[05:01] Mas para nós fazermos isso, nós precisamos colocar -- e o comando que nós queremos executar. Então nós apertamos a tecla “Enter” - e nós estamos no container, nós estamos no terminal dentro do container do nosso pod.

	kubectl exec -it portal-noticias -- bash

[05:16] E nós conseguimos executar comandos. Como, por exemplo, um curl, para enviarmos uma requisição. Eu quero enviar uma requisição para o meu localhost, ou seja, para o endereço dentro do meu pod, dentro do meu container.

	curl localhost

[05:30] Se eu apertar a tecla “Enter”, repare que ele exibiu todo o conteúdo da página web que eu esperava. Mas se nós voltarmos no nosso navegador, ele não conseguiu acessar essa página, ele demorou muito a responder; nós não conseguimos acessar.

[05:44] Mas por que nós não conseguimos acessar? Se nós voltarmos mais uma vez no nosso comando - vamos sair do nosso pod, do nosso container, apertando as teclas “Control + D”, vamos descrever ele mais uma vez, kubectl describe pod, e vamos exibir as informações do nosso portal de notícias.

[06:04] *Esse IP que ele está exibindo (10.1.0.9) é o IP desse pod, realmente - mas esse pod, esse IP especificamente, é para acesso só dentro do cluster. Então as outras aplicações dentro do cluster vão conseguir se comunicar com esse pod através desse IP.*

[06:25] *E mais, nós não fizemos nenhum tipo de mapeamento para exibirmos o nosso container dentro do nosso pod porque, como nós vimos, o IP é do pod, e não do container.*

[06:37] Como ele sabe que a partir desse IP ele deve acessar o nosso container dentro do pod? Nós precisamos fazer um mapeamento para isso - e mais, nós precisamos fazer a liberação para que esse IP seja acessível no mundo externo ao cluster.

[06:52] E para isso, nós vamos começar a estudar um novo curso, um novo conceito do Kubernetes a partir da próxima aula, em que nós vamos começar a expor a nossa aplicação para o mundo externo para que nós consigamos acessar ela. Para isso, nós vamos terminar esse vídeo por aqui e no próximo nós começaremos. Eu vejo vocês lá. Até mais!
** Expondo pods com services
*** Conhecendo services
AP: Ver imagens "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula4_video1_imagem1.png" até "aula4_video1_imagem4"
		Ver arquivo na pasta: "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/aula3/"

[00:00] Falei para vocês que nós conseguimos fazer a comunicação entre diferentes pods dentro do nosso cluster. Então, por exemplo: se nós temos esse pod de IP 10.0.0.1, nós conseguimos normalmente nos comunicar com outro pod de IP 10.0.0.2 dentro do nosso cluster.

[00:18] Mas essa comunicação está sendo bem simples, entre dois pods dentro do nosso próprio cluster. Se nós tivéssemos um cenário um pouco mais bem elaborado, onde nós teríamos um pod responsável pelas aplicações de login com esse IP terminado em .1, um de busca com .2, um de pagamentos com .3, um de carrinho com .4 e todos esses pods se comunicariam através dos seus respectivos IPs.

Dois ícones de legenda "pod" conectados por uma linha, um com IP 10.0.0.1 e o outro 10.0.0.2

[00:44] Mas vamos supor que esse pod do carrinho parasse de funcionar, ou seja, ele vai precisar ser substituído. Então criamos um novo pod para o carrinho. Só que nós não temos a garantia de que esse pod vai ter exatamente o mesmo IP do anterior.

[01:04] Porque se nós viermos no nosso terminal, o que nós conseguiríamos fazer? Nós temos mais uma vez. Deixe-me ver esse para vocês do nosso kubectl get pods. Nós temos o nosso “portal-noticias” que se nós, ao invés de descrevermos ele, utilizarmos esse comando get pod –o para formatarmos o nosso output de maneira “wide”, nós teríamos que o IP dele de 10.1.0.9.

	kubectl get pods -o wide

Sistema interligado de "Login" com pod 10.0.01, "Busca" com pod 10.0.0.2, "Carrinho" com pod 10.0.04 e "Pagamentos" com 10.0.0.3. Fora do sistema, está outro ícone de "Carrinho" com pod 10.0.0.5

[01:28] Se nós deletarmos esse nosso pod com o comando kubectl delete –f e passarmos o nosso arquivo de definição para ele - que é o nosso .\portal-notícias.yaml - ou até mesmo, nós deletarmos com o comando kubectl delete pod portal-noticias - que é o nome do nosso pod; ele vai ser removido. Nenhum mistério até aí.

	kubectl delete pod portal-noticias

[01:50] Mas se nós criarmos ele de novo... Vamos executar o comando kubectl apply -f e passar o nosso .\portal-noticias.yaml.

kubectl apply -f portal-noticias.yaml

[01:58] Se nós escrevermos um get pod -o wide de novo, repare, o IP veio diferente. Nós não temos controle sobre isso. Então se nós voltarmos para a nossa apresentação, nós estamos caindo exatamente nesse mesmo problema.

(abaixo, ver imagem: aula4_video1_imagem2.png)
[02:11] Como esses pods, que se comunicavam com esse pod , vão saber que eles devem se comunicar com esse pod novo? Como eles sabem o IP do pod novo? Essa é a pergunta que nós queremos responder agora.

[02:25] *E para isso nós temos um recurso maravilhoso dentro do Kubernetes, chamado service, ou SVC. Eles são capazes de nos fazer essas coisas. Eles são uma abstração que expõem as aplicações executadas em um ou mais pods e nós permitirmos a comunicação entre diferentes aplicações de diferentes pods e com isso eles provêm IPs fixos*.

Sistema interligado de "Login" com pod 10.0.0.1, "Busca" com pod 10.0.0.2, "Carrinho" com pod 10.0.04 e "Pagamentos" com 10.0.0.3. Ao lado, está a pergunta "Como os pods sabem o IP do pod novo?"

[02:50] *Então, o IP que nós vamos utilizar para comunicarmos diferentes pods não vai ser o IP do próprio pod, e sim o IP do nosso serviço (AP:SVC)*. Os serviços sempre vão possuir um IP fixo, que nunca vai mudar. Além disso, um DNS que nós podemos utilizar para nos comunicar entre um ou mais pods. Olhe que legal!

[03:11] E inclusive, eles são capazes também de fazer o balanceamento de carga. Então, como assim? O que isso muda na prática? Se nós voltarmos para aquele exemplo anterior, entre a comunicação do nosso pod de IP terminado em 1 e o terminado em 2, a questão é que nós não vamos nos comunicar com esse pod .2 diretamente.

[03:32] O nosso pod vai fazer comunicação com o serviço que tem esse DNS ou esse IP que nunca vão mudar, eles são estáveis; então nós temos a garantia que por mais que o IP desse pod mude, ele vai continuar sendo o mesmo, sempre sendo comunicado por causa do nosso serviço.

(AP: ver imagem citada no começo do tópico)
Ícone de "Login" com pod 10.0.0.1 ligado ao ícone de "SVC" de primeiro-serviço 10.105.147.3 ligado ao ícone de "Busca" com pod 10.0.0.2

[03:51] Então nós precisamos entender que os serviços têm esses três tipos:
   - ClusterIP
   - NodePort
   - LoadBalancer
cada um com uma finalidade específica.

[04:04] E nos próximos vídeos nós vamos entender e vai aplicar um ClusterIP, um NodePort e um LoadBalancer.

[04:11] Nós vamos entender na prática como utilizamos os serviços para mantermos uma comunicação estável entre todos os nossos pods, entre os nossos recursos dentro do nosso cluster.

[04:20] Então por esse vídeo é só isso! Nós já entendemos qual é o problema e quem vai resolver ele - que são os services. A partir de agora nós vamos implementar, nós vamos criar esses services de maneira também declarativa para resolver os nossos problemas, entendendo cada um desses três tipos : o ClusterIP, o NodePort e o LoadBalancer.
*** Criando um ClusterIP
AP: Ver imagens em: "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula4_video1_imagem1.png" à "aula4_video1_imagem6"
    Ver aquivos em: "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/aula4/"

[00:00] O primeiro tipo de serviço que nós vamos abordar dentro do Kubernetes é o ClusterIP.

(Imagem 1)
Ao lado, está a área delimitada de "Cluster" contendo o sistema interligado de quatro ícones de "pod" com os números "10.0.0.1", "10.0.0.2", "10.0.0.4" e "10.0.0.3".

[00:05] *E qual é o propósito dele? Para que ele serve? Ele serve para nada mais, nada menos, que fazer a comunicação entre diferentes pods dentro de um mesmo cluster.*

[00:15] Então, nesse cenário que nós estamos visualizando, todo e qualquer pod. Esse de final .2, .4 e .3 eles vão conseguir fazer a comunicação para este pod de final .1 a partir desse serviço, utilizando o IP e o DNS, ou o DNS no caso desse serviço.

[00:35] *E vale ressaltar que o serviço não é uma via de mão dupla, não é porque este pod tem um serviço que ele vai conseguir se comunicar com os outros que não têm também, porque eles não têm o serviço atrelado a eles. Então unilateralmente falando, todos os outros vão se comunicar a este pod de maneira estável, mas ele só porque é um serviço não vai se comunicar aos outros se eles também não tiverem.*

[01:00] Tendo isso em mente, se nós tentarmos acessar esse pod a partir de fora do cluster, o que vai acontecer? Utilizando esse serviço, claro, ClusterIP, nós não vamos conseguir, porque a comunicação, como eu falei, é apenas interna do cluster utilizando um ClusterIP.

(imagem 2)
Mesma imagem anterior, mas abaixo do ícone de "SVC" há a figura de um computador com uma seta com um "x" em cima indicando para a área de "Cluster". Ao lado de "SVC", esta escrito "Apenas para comunicação interna do cluster!". Dentro da área delimitada de "Cluster", há um pequeno ícone de "SVC" ao lado do primeiro pod de número 10.0.01.

[01:18] Então vamos começar na prática! Nós vamos criar de início dois pods para fazermos o nosso experimento com o ClusterIP. O que nós vamos fazer imediatamente? Nós vamos primeiro criar um arquivo de definição para esse nosso primeiro pod, o nosso “pod-1-antes.yaml”.

[01:36] E vamos definir todo ele, a versão da API; nós vamos definir o tipo, que é um pod; no metadata nós vamos definir o nome dele, nós vamos chamar ele de pod-1 assim como o nome do arquivo. Isso não é obrigatório, só frisando.

[01:52] E nas especificações nós vamos colocar as informações do container que vai compor esse pod, que vai ter um nome também não relevante para nós nesse cenário, mas é sempre bom nós definirmos semanticamente. Vou colocar ele como container-pod-1 e a imagem que ele vai utilizar ainda vai ser do nginx:latest.

[02:13] Dito isso, nós vamos dar um pequeno parêntese. Caso você esteja olhando para esse arquivo como desenvolvedor, se você não soubesse, olhando na documentação do nginx no Docker Hub, que ele é executado na porta 80 por padrão, como você poderia saber que este container definido dentro desse pod está escutando na porta 80?

[02:38] A boa prática em questão de documentação seria nós definirmos através desse campo ports e colocarmos dentro a instrução também: containerPort, indicando que este container definido dentro deste pod está ouvindo na porta 80.

[02:55] Então quando o pod for criado e tiver um IP atribuído a ele, se nós tentarmos fazer essa requisição na porta 80, nós vamos cair no nosso nginx.

[03:06] Tendo isso já pronto, nós podemos criar o nosso segundo pod. Então a mesma ideia vai ser aplicada. Eu vou copiar e vou criar um novo arquivo chamado “pod-2.yaml”, vou colar e vou trocar para pod-2, para manter o mesmo nome padronizado no container também.

[03:27] E ele também está exposto na porta 80. Por quê? Não vai dar problema isso? Porque os dois são pods diferentes e cada um tem o seu respectivo IP, então não vai ter nenhum conflito em relação a isso.

[03:39] Vou salvar os dois arquivos e agora nós vamos criar esses dois pods, com o comando kubectl apply -f .\pod-1-antes.yaml e logo depois também o nosso pod-2.

[03:55] E agora o que nós temos, se nós voltarmos na nossa apresentação? Nós temos o nosso “Cluster”, o nosso portal de notícias em execução, o nosso “pod-1” e o nosso “pod-2” também.

[04:07] Só que, falta o que? Nós termos o nosso serviço. Nesse cenário que nós estamos testando o nosso cluster pela primeira vez a ideia vai ser que esse serviço pod-2 seja voltado apenas ao pod-2.

(Imagem 3)
Área delimitada "Cluster" com um retângulo tracejado. Dentro, está o ícone de "pod-1", outro pod de "portal-noticias", outro de "pod-2" e um quarto ícone "SVC" de "svc-pod-2".

[04:22] Então nós queremos criar uma maneira estável de comunicarmos com o nosso segundo pod, então vamos criar esse serviço para nós entendermos como isso funciona.

[04:32] *Assim como nós temos o recurso do pod dentro do Kubernetes, nós temos o recurso de service, de serviço. Como nós queremos criar esse recurso, nada mais válido do que nós criarmos um arquivo de definição. Então vamos criar o nosso “svc-pod-2.yaml”, o nome do arquivo.*

[04:52] E dentro dele nós vamos continuar utilizando a versão 1 da API, nada vai mudar até então. Quando mudar, eu vou destacar isso para vocês e o tipo que nós queremos criar.

[05:02] *É um pod? Não é mais um pod, é um serviço (AP: por isso: "kind: Service"). Olhe que legal! E nós vamos definir no metadata dele o quê? Também um nome, então nós podemos chamar ele de svc-pod-2 e também uma especificação.*

[05:19] *E dentro dessa especificação nós também não vamos definir containers, porque ele não é mais um pod. Nós vamos definir o tipo. Qual é o tipo do serviço que nós estamos criando? É um ClusterIP.*

[05:33] E agora, o que nós temos? Se nós salvássemos isso agora, tecnicamente, na teoria nós já temos o nosso serviço. Só que, o que acontece? Quando o nosso pod-1 ou o nosso portal de notícias quiserem se comunicar com o nosso pod-2, ele precisa encaminhar essas requisições que ele receber para o nosso pod-2.

(imagem 4)
Mesma imagem anterior, mas Os ícones de "pod-1" e "portal-noticias" se conectam por uma seta ao ícone de "SVC-pod-2", o qual se conecta por uma seta a "pod-2".

[05:56] Só que, como ele sabe que ele deve se comunicar com o pod-2? Como ele sabe que, isso se refere a isso ?

(imagem 5)
Mesma imagem anterior. Porém, no canto superior direito do retângulo tracejado, está o escrito "Labels!". Ao lado do ícone de "SVC", está a etiqueta escrita "selector: app: segundo pod", e ao lado do ícone de "pod-2" está a etiqueta escrita "app;segundo-pod".

[06:11] *Caso você esteja pensando, não é pelo nome, o nome é completamente irrelevante nesse caso. Nós precisamos ter uma maneira sólida e estável de fazermos essa atribuição. Esse serviço está selecionando este recurso, e para isso nós temos as labels - lembra que eu falei delas para vocês? Nós vamos usar elas agora!*

[06:33] Então nós podemos e devemos, nesse cenário, etiquetar o nosso recurso - por exemplo: o nosso pod-2 - e informarmos que este serviço seleciona apenas os recursos que possui essa label.

[06:47] E como isso funciona no nosso arquivo declarativo? Basta nós virmos e definirmos dentro do nosso metadata as labels que nós queremos utilizar, através de uma chave. Nesse caso, "app", que nós estamos chamando e um valor que nós definimos como "segundo-pod".

[07:04] E nós também temos a liberdade de utilizarmos quantas e quaisquer label nós quisermos, então qualquer chave com qualquer valor nós podemos definir sem nenhum problema. Nós podemos colocar diversas coisas.

[07:22] Mas nesse caso o importante é mantermos sempre a semântica, a informação do que realmente está sendo feito .

[07:28] E agora com a nossa label criada (app), a nossa chave com este valor "segundo-pod", nós precisamos informar para este serviço que ele vai selecionar todos os recursos que tiverem esta chave "app" com o valor "segundo-pod". Olhe que legal!

[07:48] Então a partir desse momento ele já sabe que quando ele estiver recebendo alguma requisição, ele deve encaminhar para o nosso "segundo-pod", o nosso "pod-2".

[08:02] Só que outra pergunta: agora, como ele sabe que ele deve despachar a requisição que ele receber para a porta 80 do nosso pod? Porque como nós vimos, o que está sendo exposto dentro desse pod (no pod-2.yaml) é a porta 80, mas não tem nada claro para esse nosso serviço que ele deve, assim que receber uma requisição, encaminhar ela para a porta 80.

[08:27] É claro então que nós precisamos definir também configurações de porta dentro - e isso é bem fácil: basta nós definirmos do nosso port, definirmos a instrução "port" e informarmos qual é a porta que nós queremos ouvir e qual é a porta que nós queremos despachar.

[08:49] Isso significa o quê? Que nós já sabemos em qual porta nós estamos soltando a nossa requisição. Mas em que porta o nosso serviço está ouvindo? Porque ele vai ter um IP, mas ele vai ter também uma porta para receber essas requisições. Então nós precisamos, e devemos, nesse cenário também definirmos uma porta onde esse serviço vai escutar.

(imagem 6)
Mesma imagem anterior. Porém, ao lado de "SVC", está a pergunta "Qual a porta que esse serviço escuta?". Na seta que conecta "SVC" ao "pod-2", está o valor ":80".

(AP: *se nós definirmos só a port, implicitamente ele vai nos definir também o TargetPort sendo igual ao port? Então nós não precisamos explicitar o TargetPort se nós explicitarmos só o port, ele assume que os dois são iguais se nós definirmos só o primeiro.*)

[09:13] Mas olhe que legal: se nós definirmos a nossa porta - e nós temos a liberdade de definirmos a porta de entrada igual a porta de saída – então, o que nós estamos fazendo? Nós estamos falando que o nosso serviço vai receber as requisições na porta 80 e vai despachar para a porta 80 também. De quem? De qualquer recurso que tiver a label app segundo pod.

[09:39] Vamos entender isso na prática. Agora nós vamos criar esse recurso efetivamente, vamos atualizar primeiro o nosso "pod-2", porque nós definimos essa label para ele, ou seja, agora ele foi configurado.

[09:54] Se nós viermos em "kubectl describe pod pod-2", olhe só, em cima - ele tem as nossas labels, : labels: app-segundo-pod. Que legal!

[10:08] E se nós agora criarmos o nosso serviço também com 

	kubectl apply -f svc-pod-2-antes.yaml

ele foi criado.

[10:17] Assim como nós temos o comando kubectl get pods, nós temos o comando 

	kubectl get service
ou 
	kubectl get svc

os dois funcionam.

AP: Saida do meu terminal
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP    3h12m
svc-pod-2    ClusterIP   10.111.33.72   <none>        80/TCP     16s

[10:25] E ele vai nos mostrar esse nosso serviço. Esse primeiro "kubernetes" já vem por padrão criado com o nosso cluster. Esse "svc-pod-2" é do tipo "ClusterIP", ele tem um IP que foi definido ali no momento da criação dele, ele não tem nenhum IP externo e a porta que ele ouve é a porta 80 e vai ser a porta também que ele vai despachar.

[10:50] Então, como isso vai funcionar agora? Como nós nos comunicamos com o nosso pod-2? Vamos fazer o seguinte: eu vou digitar um kubectl get pods, nós temos o nosso pod-1 e o nosso portal de notícias (AP: o portal de noticias vem da execução da aula anterior). Vamos fazer o seguinte: eu vou digitar um kubectl exec -it pod-1 e vou entrar nele com um bash.

	kubectl exec -it pod-1 -- bash

[11:11] O que eu quero fazer agora é enviar uma requisição. Vou fazer um curl para nós pegarmos essa página que nós queremos adquirir. Para onde? Para que o nosso endereço IP do nosso ClusterIP, que é 10.111.33.72. Onde? Na porta 80.

	curl 10.111.33.72:80

[11:32] E olhe só que legal: está o nosso retorno do nginx. Se nós tentarmos fazer a mesmíssima coisa a partir do nosso portal de notícias, o que vai acontecer? Vamos lá: curl 10.111.33.72:80. A mesma coisa, que legal! Passei até batido, que legal!

[11:58] E agora o ponto é o seguinte: eu vou sair de dentro também do nosso pod, do nosso container, vou limpar a nossa tela e vou fazer o seguinte. Eu vou digitar kubectl delete –f e vou deletar o nosso pod-2.

[12:15] Mas o serviço vai continuar em execução no nosso cluster IP. Não é à toa que se eu executar agora um kubectl get svc, ele vai continuar ouvindo na porta 80.

[12:28] Se eu tentar mais uma vez executar esse curl que eu acabei de fazer para a porta 80 deste serviço, ele vai continuar ouvindo, mas ele não vai ter lugar nenhum para despachar porque não tem ninguém ouvindo na porta 80. Olhe que triste!

[12:44] Então, isso significa que se em algum momento nós criarmos qualquer outro pod. Por exemplo: o nosso pod-2 de novo (com essa label que ele vai ser selecionado pelo serviço), independentemente do IP dele ser diferente (AP: ou seja: quando nós matamos o pod-2 e subimos ele denovo, ele sobe com o IP diferente do que tinha antes), que nós vimos que vai ser (AP: como a prática de matar pods e subi-los novamente - que vemos que sempre sobem com outro IP), o comando vai continuar funcionando; porque agora o nosso serviço tem um IP estável, DNS estável para fazer essa comunicação.

[13:12] Se nós tentarmos, inclusive, também fazer a comunicação via DNS, também vai funcionar. Então, um último comentário também para ficar bem direto e bem passado o que eu quero passar para vocês é que dentro da configuração de porta nós temos a liberdade de definirmos que a porta em que nós vamos ouvir é diferente da porta que nós queremos despachar.

[13:38] Como assim? nós vamos continuar despachando na porta 80, mas ao invés do nosso serviço ouvir na porta 80, ele pode ouvir em qualquer outra porta. Então basta nós definirmos, por exemplo, a porta 9000. Nós temos essa liberdade.

(AP: para isso, ver agora a service: "svc-pod-2-depois.yaml")

[13:55] *E ao invés do nosso pod ouvir na porta 9000, nós sabemos que ele está ouvindo na porta 80. Então como a porta que o nosso serviço ouve é diferente da porta que nós queremos ouvir no nosso pod, nós devemos definir também então um outro campo chamado "TargetPort" - que nesse caso é o 80. Qual é a porta que nós queremos despachar o nosso serviço? A porta 80.*

[14:23] Então se nós salvarmos e executarmos, nós vamos configurar o nosso serviço novamente. Olhe o que que vai acontecer, vamos lá! Ele foi devidamente configurado. Se nós escrevemos kubectl get svc, repare que agora ele não ouve mais na porta 80, ele ouve na porta 9000.

NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP    3h33m
svc-pod-2    ClusterIP   10.111.33.72   <none>        9000/TCP   21m


[14:44] Mas o IP é exatamente o mesmo, a diferença é que agora quando nós fizermos alguma requisição, por exemplo, a partir do nosso portal de notícias para esse pod-2, nós não vamos mais enviar requisição para a porta 80; nós vamos enviar ela para a porta 9000 e tudo vai continuar funcionando.

[15:04] Então, o que acontece ? Quando nós temos o nosso pods - eu vou botar o - wide para nós vermos o nosso IP - o nosso pod-2 tem este IP que ouve na porta 80, que é onde está a nossa aplicação do nginx.
NAME              READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES
pod-1             1/1     Running   0          22m     10.244.0.8    minikube   <none>           <none>
pod-2             1/1     Running   0          7m37s   10.244.0.11   minikube   <none>           <none>
portal-noticias   1/1     Running   0          11m     10.244.0.10   minikube   <none>           <none>


[15:19] Vou até abrir mais um texto para nós entendermos. Nós temos o nosso pod no IP 10.244.0.11 ouvindo na porta 80, nós conseguimos nos comunicar a esta aplicação usando este endereço (AP: por exemplo: fazendo uma requisição do bash à partir da pod-1: curl 10.244.0.11:80 ele encontra a html). Mas qual é o problema dela? O problema é que ela não é estável.

[15:46] Então nós temos total liberdade para fazermos isso, só que se nós tentarmos também nos comunicar agora a partir do IP do nosso serviço, que é 10.111.33.72, o que vai acontecer? Nós precisamos fazer essa comunicação a partir da porta como nós definimos agora, 9000 e ele vai fazer o bound, ele vai fazer esse bind para nós, para o nosso 10.244.0.11 na porta 80.

[16:19] Então nós também temos a possiblidade de variarmos essa porta, como nós fizemos e da maneira como nós quisermos, contanto que ele esteja livre para este IP e ele vá fazer esse redirecionamento para a nossa "TargetPort" definida do nosso container, dentro do nosso pod.
*** Criando um Node Port
AP: ver imagens "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula4_video3_imagem1.png" e "aula4_video3_imagem2.png"

[00:00] *Tendo entendido o que são ClusterIP, fica muito mais fácil nós entendermos do que que se trata um NodePort. Eles nada mais são do que um tipo de serviço que permitem a comunicação com o mundo externo.*

[00:14] Então agora nós conseguimos fazer uma requisição, enviar uma requisição de uma na que não está dentro do nosso cluster para o nosso cluster, para algum pod dentro dele.

[00:26] Então significa que agora nós conseguimos acessar, por exemplo, a partir do navegador alguma aplicação que está dentro do nosso cluster, utilizando o nosso NodePort.

[00:34] *E ele vai além disso, ele também funciona dentro do próprio cluster como um ClusterIP. Então se você quer ter algum pod que além de ser acessado dentro do cluster, também deve ser acessado de maneira externa, você pode utilizar o NodePort, porque ele também vai funcionar como ClusterIP.*

[00:53] Isso significa que, por exemplo, este pod, que tem a label version 2.0, consegue ser acessado tanto por esse pod de dentro do cluster a partir desse serviço, quanto fora do nosso cluster, também a partir desse serviço.

[01:09] Então agora nós vamos conseguir fazer toda a criação do nosso NodePort. Nós vamos deixar posteriormente tudo bem elaborado com o projeto. Como eu falei para vocês, nós vamos alcançar o estado onde nós conseguimos gerenciar múltiplos pods com o mesmo serviço, tudo a partir das nossas labels e com o balanceamento de carga automático. Mas vamos com calma, vamos primeiro criar o nosso NodePort na primeira vez.

[01:36] Qual é a ideia ? Nós já temos o nosso cluster do jeito que ele está agora, nós temos o nosso pod-1, o nosso pod-2, o nosso portal-noticias e um serviço que faz essa requisição esse tratamento de requisição para enviar para o nosso pod-2 - tudo isso feito através das nossas labels que nós criamos.

(imagem 1)
Ícone de "SVC" com legenda "NodePort" ao lado do texto "Abre comunicação para o mundo externo" sobre um computador com uma seta indicando para a área tracejada de "Cluster". Dentro desta, há o "selector:" de "version: 1.0" sobre o ícone de "SVC" conectado a três pods de "version 1.0", e outro "selector:" de "version: 2.0" com ícone de "SVC" conectado a um pod e a outro pod de "version: 2.0". Ao lado, há o texto "NodePorts também funcionam ClusterIPs"

[01:56] A ideia agora vai ser bem parecida, só que nós vamos querer criar um serviço para o nosso pod-1, onde ele vai expor o nosso pod-1 para o mundo externo. Então, agora nós precisamos, mais uma vez, voltar ao nosso Visual Studio Code. Nós já temos o nosso pod-1 e o nosso pod-2, o nosso portal-noticias também e o ClusterIP criado anteriormente já rodando.

(imagem 2)
Área tracejada de Cluster contendo o ícone de "svc-pod-1" vindo de fora deste e ligado ao "pod-1" ligado ao "svc-pod-2", que por sua vez está ligado pela porta ":80" ao "pod-2". O ícone pod de "portal-noticias" se conecta ao "svc-pod-2".

[02:20] A ideia agora vai ser nós criarmos o nosso service chamado NodePort desse tipo. A ideia é bem parecida, vamos chamar então de name: svc-pod-1 porque esse serviço vai ser voltado para o nosso pod-1.

[02:36] E nós vamos definir a versão da API também como V1. Nada de novo, o tipo ainda é um serviço, um service, então escrevemos Service .

[02:48] Na metadata vamos dar um nome para ele, vamos seguir a mesma ideia que nós colocamos no anterior que foi "svc-pod-2". nós vamos colocar também "svc-pod-1".

[02:59] *Nas especificações, olhe só como é bem parecido: o tipo, ao invés de ser ClusterIP, vai ser um NodePort. Olhe que legal!*

[03:10] E dentro nós também vamos ter aquelas configurações de porta. Vamos definir, qual é a porta que, como eu falei para vocês, esse serviço, o nosso NodePort também vai funcionar como ClusterIP.

[03:24] Então, de maneira similar ao nosso serviço 2, nós também vamos definir um port dentro. Qual é a porta em que o nosso serviço vai ouvir dentro do cluster? Nós queremos, por exemplo, que seja na porta 8080. Nós temos total liberdade para isso.

[03:45] Vamos colocar só port: 80. Lembra que eu falei para vocês que *se nós definirmos só a port, implicitamente ele vai nos definir também o TargetPort sendo igual ao port? Então nós não precisamos explicitar o TargetPort se nós explicitarmos só o port, ele assume que os dois são iguais se nós definirmos só o primeiro.*

[04:09] Então, agora nós já definimos o nosso port. Se nós tentarmos executar para valer, ele vai funcionar a princípio. Vamos ver, eu vou salvar, vou no nosso terminal vou digitar 
	
	kubectl apply -f svc-pod-1-antes.yaml

[04:31] Se nós apertarmos a tecla “Enter”, ele vai ser criado. Mas ainda faltam alguns pequenos detalhes. Como, por exemplo: nós temos o nosso serviço do tipo NodePort, e nós precisamos, assim como nós fizemos anteriormente, fazer o bound desse serviço com este pod? Então, vamos colocar as labels, no caso, vamos seguir a mesma ideia de, por exemplo: app e vamos chamar ele de primeiro-pod para seguirmos o mesmo padrão que nós viemos fazendo.

[05:08] E nós vamos adicionar fora de port alinhado, o seletor. Então: selector: e vamos chamar o nosso app: primeiro-pod.

[05:22] Então agora, como isso vai funcionar ? Se nós voltarmos e configurarmos os dois da maneira correta... Configuramos o nosso serviço e agora nós configuramos também o nosso pod. Devidamente configurado!

[05:41] E se nós tentarmos, como eu falei para vocês, fazer o acesso a partir de dentro do cluster, nós vamos conseguir. Então, vamos lá!

[05:48] Vamos digitar "kubectl get svc". 
AP: meu resultado no terminal:
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        20h
svc-pod-1    NodePort    10.107.54.58   <none>        80:31977/TCP   10m
svc-pod-2    ClusterIP   10.111.33.72   <none>        9000/TCP       17h

Está o nosso svc-pod-1, ele tem esse IP e olhe só como ele nos mostra que ele faz o bound da porta 80 para a porta 31977. O que isso quer dizer? Nós vamos entender, com calma.

[06:07] Primeiro nós vamos fazer o mesmo teste que nós fizemos com o ClusterIP. Vamos acessar ele a partir do nosso portal de notícias. Então, docker não, kubectl exec –it. Vamos executar o nosso portal-noticias em modo interativo e o bash.

	kubectl exec -it portal-noticias -- bash

[06:25] Se nós colocarmos, fazer um curl novamente para 10.107.54.58, que é o nosso IP na porta 80, o que vai acontecer? Mágica! Tudo continua funcionando sem nenhum problema!

[06:46] Mas como nós fazemos para acessar agora esse NodePort a partir do mundo externo, a partir do nosso navegador? Então vou abrir uma nova aba. Vamos lá, o que vai acontecer ?

[06:57] Se nós tentarmos acessar esse serviço... Vamos colocar o IP dele, vamos pegar 10.107.54.58 e vamos colocar ele na porta 80. O que vai acontecer pessoal? Ele está carregando e mais uma vez aparentemente está demorando demais e não vai conseguir.

[07:18] Por quê? Porque olhe só a peculiaridade. Vou limpar a nossa tela e vou apertar as teclas “Ctrl + D” para sair de dentro do container. Vou digitar get svc de novo, para nós destrancarmos melhor.

[07:30] Nós temos o nosso IP para esse svc-pod-1, mas repare na coluna que ele está:  "CLUSTER-IP".

[07:36] O que isso quer dizer? Quer dizer que esse IP é para comunicação dentro do cluster. Então qual é o IP que eu devo utilizar para fazer a comunicação a partir de fora do cluster? Eu tenho que fazer isso a partir do IP do meu nó, porque é um NodePort.

[07:55] Então se eu vier e fizer 

	kubectl get nodes -o wide 

AP:minha saída:
NAME       STATUS   ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE               KERNEL-VERSION   CONTAINER-RUNTIME
minikube   Ready    control-plane   20h   v1.28.3   192.168.59.100   <none>        Buildroot 2021.02.12   5.10.57          docker://24.0.7


para ele botar o IP, olhe só - o nosso external IP no caso do Windows é none e o nosso IP interno é 192.168.59.100.

[08:13] No caso do Windows, agora é um momento em que nós vamos ter uma pequena diferença entre o pessoal que está no Windows e no Linux, porque no caso do Docker Desktop no Windows ele faz um bound automaticamente do Docker Desktop para o nosso LocalHost, então o IP desse nó no Windows vai ser LocalHost.

[08:33] Então se nós viermos no nosso navegador e colocarmos LocalHost na porta 80, nós vamos a princípio acessar, só que não é isso que nós queremos. Isso é o Windows que tem alguma coisa rodando na porta 80 para nós. O que nós queremos acessar é a página do nginx.

[08:53] Mas eu botei, não botei pessoal!? A porta 80? *Por que eu não estou conseguindo acessar? Por que isso não funciona? Porque, na verdade, se nós formos um pouco mais "malandros", nós vamos observar que a porta 80 é a de uso interno do cluster, mas ele faz o bound para a porta 30363 - que é aquela porta louca que nós vimos.*

[09:16] Então se nós copiarmos esse número, pegarmos esse 30363 e colocarmos LocalHost nessa porta – mágica! Nós conseguimos agora a nossa aplicação através do nosso serviço de maneira externa.

[09:31] Mas tem uma peculiaridade: esse (AP: número que o kubectl definiu para porta do nosso svc-pod-1) número é arbitrário, ele vai variar de 30000 até 32767. Mas nós temos a liberdade para nós definirmos o NodePort que nós queremos utilizar (*AP: assim podemos padronizar o número de nossas portas, não deixando o kubectl escolhe-las aleatoriamente para nós*)

[09:51] Então vamos fazer o seguinte: nós podemos voltar no nosso serviço que nós acabamos de definir e definirmos também uma instrução, um outro campo chamado NodePort, onde nós podemos definir qualquer valor no intervalo de 30000 até 32767.

[10:09] Nesse caso vou colocar, por exemplo, o próprio 30000 (AP: ver no arquivo "svc-pod-1-depois.yaml" que foi definido um novo campo: "nodePort: 30000"). No momento em que eu aplicar a minha mudança a esse serviço, olhe o que vai acontecer.
(AP: Antes de seguir abaixo eu tenho que recarregar o svc-pode-1 no arquivo com essa porta 30000:
	kubectl apply -f svc-pod-1-depois.yaml
)

(AP: A fala abaixo é da execução no windows - ele colocou no navegador localhost:30000 e conseguiu carregar - no meu Linux não carrega quando escrevo essa url... mas apenas colocando o IP:30000... pouco abaixo ele menciona que é isso que devemos fazer no linux, e não via "localhost:30000")
[10:20] Ele foi configurado! Se nós digitarmos get svc de novo, olhe só, localhost:30000. Então se nós viermos e executarmos na porta 30000, repare que tudo continua funcionando.

[10:34] Agora pessoal, repare que tudo, da maneira como nós esperávamos e que nós vamos fazer agora. Eu vou dar uma pequena pausa, nós vamos cortar esse vídeo e eu vou entrar no Linux para o pessoal que também está no Linux entender como tudo funciona sem nenhum problema.

[10:49] Pessoal, agora nós estamos no Linux, com as exatas mesmas configurações, o pod-1, o pod-2, o portal-notícias, os nossos dois serviços que nós criamos. Nada de novo, os mesmos arquivos.

[11:02] E a diferença para acessarmos é que se nós viermos no nosso navegador e executarmos localhost:30000, ele não vai conseguir acessar - porque como eu falei para vocês, no Linux nós estamos utilizando o Minikube com o Virtual Box e ele não faz o bind automático para o nosso LocalHost.

[11:20] Para nós conseguirmos acessar, nós vamos executar o comando kubectl get nodes -o wide e ele vai nos retornar, nessas informações todas, o internal IP.

[11:32] E vai ser ele. no caso, o meu é 192.168.99.106 (AP: esse é o do professor do curso); no caso de vocês provavelmente vai ser diferente (AP: o meu é: 192.168.59.100). Então eu vou copiar esse IP e agora no meu navegador vou fazer o acesso através dele na porta 30000. Olhe só que legal, tudo funcionando normalmente!

[11:53] Então LocalHost não vai funcionar, nós vamos usar o nosso internal IP no Linux. Enquanto no Windows, todo o acesso vai ser via LocalHost porque ele vai bind direto. A única diferença vai ser essa, o comportamento do resto todo é exatamente o mesmo.

[12:08] Então por esse vídeo é só! NodePort, agora nós conhecemos ele e como nós podemos defini-lo e criá-lo. Eu vejo vocês no próximo vídeo, onde nós vamos falar sobre LoadBalancer. Até mais!
*** Criando um Load Balancer
AP: Ver imagens: "./Imagens-8s/aula4_video4_imagem1.png" e "aula4_video4_imagem2.png"
Arquivo de código em: "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/aula4/svc-pod-1-loadbalancer.yaml"

[00:00] Entender o que é um LoadBalancer depois que nós já entendemos do que se trata um NodePort e um ClusterIP é bem fácil - principalmente porque o *LoadBalancernada mais é do que um ClusterIP que permite a comunicação entre uma máquina do mundo externo e os nosso pods. Só que ele automaticamente se integra ao LoadBalancerdo nosso cloud provider*.

(imagem1)
Ícone proeminente de "SVC" com legenda "LoadBalancer". Ao lado, a área tracejada de "Cluster" contém os logotipos de "AWS", Google Cloud e Azure, ligados a dois ícones de "SVC". No primeiro, há conexão com três pods, e o segundo com apenas um.

[00:23] Então quando nós criamos um LoadBalancer ele vai utilizar automaticamente, sem nenhum esforço manual, o cloud provider da AWS ou do Google Cloud Platform ou da Azure, e assim por diante.

(imagem 2)
Mesma imagem anterior, porém com o texto "Abre comunicação para o mundo externo usando o Load Balancer do provedor! ao lado do ícone poreminente de "SVC" com legenda "LoadBalancer"

[00:37] Então, vamos ! Eu vou pegar o nosso pod-1 que nós viemos trabalhando e vou criar esse mesmo pod no nosso cluster do Google Cloud Platform.

[00:48] Vou colocar o arquivo, vou criar ele com as mesmas definições que eu acabei de copiar ali, vou colar, vou digitar um apply, kubectl apply –f e passar o nosso pod-1.yaml. Ele foi criado sem nenhum problema, nós digitamos um kubectl get pods, ele foi criado e agora nós precisamos criar o nosso LoadBalancer.

[01:11] Nós vamos fazer o seguinte: vamos criar o nosso "svc-pod-1-loadbalancer.yaml" e dentro dele nós vamos definir mais uma vez a versão da nossa API como v1. O que nós queremos criar continua sendo um service e em metadata vamos chamar ele também pelo name: "svc-pod-1-loadbalancer".

[01:44] nas especificações nós vamos definir o tipo que vai ser o nosso "type: LoadBalancer", agora sem nenhum problema. em "ports:" nós vamos definir a nossa porta de entrada, onde nós podemos ir definindo. Nós queremos que dentro do cluster.

[02:02] Como ele é um NodePort, ele também é um ClusterIP, ele ouça na porta 80 e despacha também para a porta 80, dentro do cluster. E que também o nosso "nodePort : 30000", por exemplo. Nós podemos fazer essa definição.

(AP: A respeito do que foi dito abaixo em [02:19]: nós estamos editando o arquivo svc-pod-1-loadbalancer.yaml, nós vamos definir nela:
  selector:
    app: primeiro-pod
se referindo à label 
  labels:
    app: primeiro-pod
que está dentro do arquivo "pod-1-depois.yaml"
)

[02:19] Por fim, falta apenas nós selecionarmos qual é o nosso pod. Nesse caso vamos definir a "label" com a chave API e o valor "primeiro-pod".

[02:30] Tudo perfeito! Basta agora nós copiarmos essas mesma definição, vir no nosso Google Cloud Platform e criar esse arquivo que vai ser o nosso “lb.yaml”. Nós colamos sem nenhum mistério: kubectl apply -f lb.yaml e ele vai criar para nós sem nenhum problema.

(AP: Abaixo é olhando na Google Cloud Plataform)
[02:57] Se nós viermos agora dentro do nosso cluster na atividade na parte visual dele, nós conseguimos vir em “Serviços e entradas” e olhe só que legal: está - o nosso serviço que nós acabamos de criar! E mostra que tem 1 de 1 pod sendo gerenciado por ele no nosso “cluster-1”.

[03:17] Ele está terminando de criar os endpoints para acesso. Se nós continuarmos atualizando, vai ser bem rapidinho, nós vamos conseguir acessar esse nosso pod a partir do próprio navegador.

[03:28] Então se vocês estivessem assistindo agora em tempo real, vocês também conseguiriam ao mesmo tempo que eu fazer o acesso a esse pod, porque nesse exato momento ele está sendo publicado e sendo possivelmente acessado com o LoadBalancer do Google Cloud Platform - já tudo integrado sem nenhum problema, sem nenhuma configuração adicional na gestão de balanceamento de carga que acabou de ficar pronto.

[03:55] Basta nós clicarmos no link que foi gerado o do IP. Ele está alertando sobre o redirecionamento e está o nosso nginx, que é o nosso pod-1 sem nenhum problema na web. Olhe que legal e fácil, bem simples!

[04:11] Então agora que nós já nos familiarizamos com os três tipos de serviço, ClusterIP, NodePort e LoadBalancer, nós vamos colocar eles na prática em uma aula em que nós vamos trabalhar com eles em cima do nosso projeto, do portal de notícias e nós vamos sedimentar o conteúdo que nós aprendemos agora nessas últimas aulas.
*** Visão geral das aulas
O que são e para que servem os Services
Como garantir estabilidade de IP e DNS
Como criar um Service
Labels são responsáveis por definir a relação Service x Pod
Um ClusterIP funciona apenas dentro do cluster
Um NodePort expõe Pods para dentro e fora do cluster
Um LoadBalancer também é um NodePort e ClusterIP
Um LoadBalancer é capaz de automaticamente utilizar um balanceador de carga de um cloud provider
** Aplicando services ao projeto
*** Acessando o portal
AP: Utilizar arquivos portal-noticias.yaml e svc-portal-noticias.yaml da pasta: "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/Aula5/"

A ideia é subir um portal de notícias nessa aula. Nesse video foram configurados o pod e o service.

[00:07] O primeiro passo que nós vamos fazer é colocar o nosso “portal-noticias” com o NodePort para que nós consigamos acessar ele de fora do nosso cluster.

Primeiro foram interrompidos os pods e services que estavam em execução:

	kubectl delete pods -all

	kubectl delete svc -all

Depois subiu o pod e o service:

 kubectl apply -f portal-noticias.yaml 

 kubectl apply -f svc-portal-noticias.yaml

Não repeti as explicações das configurações feitas nos arquivos portal-noticias.yaml e svc-portal-noticias.yaml pois são as mesmas feitas em outra aulas.
*** Pergunta da Alura
João escreveu um arquivo YAML para criar um service no Kubernetes que exponha um portal de notícias. Ele definiu o nome do serviço, estabeleceu o tipo como “NodePort” e a porta como “80”. No entanto, ele não tem certeza se o arquivo está correto e precisa de ajuda.

apiVersion: v1
kind: Service
metadata:
  name: svc-portal-noticias
spec:
  type: NodePort
  ports:
    - port: 80

Levando em conta este arquivo YAML, qual alternativa traz a afirmação correta?

**** Ele não funcionará, pois não definimos o campo targerPort dentro de ports:.
Alternativa errada! Se definirmos apenas o campo port, o valor de targetPort será o mesmo.

**** Ele não funcionará, precisamos definir labels dentro do metadata do service.

**** Ele funcionará sem problema algum.
Alternativa correta! O campo nodePort e targetPort serão definidos implicitamente.

**** Ele não funcionará, pois não definimos o campo nodePort dentro de ports:.
Alternativa errada! Ele funcionará e atribuirá uma porta aleatória entre 30000 e 32767.
*** Subindo o sistema
AP: ver: "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula5_video2_imagem1.png"  e "aula5_video2_imagem2.png"
	ver arquivos: "sistema-noticias.yaml" e "svc-sistema-noticias.yaml" da pasta "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/Aula5/"

[00:00] O que nós temos até então é o nosso portal de notícias sendo gerenciado por esse serviço do tipo NodePort, que permite o acesso do mundo externo ao nosso pod dentro do nosso cluster.

(Imagem 1)
Imagem com figura de computador que se conecta por uma seta à área de "Cluster" com o ícone "svc-portal-noticias" de "NodePort", o qual se conecta ao pod de "portal-noticias".

[00:11] Mas o que nós queremos? Como nós falamos, criar um serviço e um pod responsáveis no caso pelo sistema de notícias onde nós vamos cadastrar. Esse sistema também vai prover para o nosso portal essas notícias para que nós possamos exibir.

[00:25] Então, como nós queremos acesso do mundo externo ao nosso pod do sistema de notícias e também ao mundo interno do nosso cluster, para que o nosso portal consiga consumir essas notícias, nós precisamos criar um NodePort e um pod no caso - obviamente com a imagem do nosso sistema. Então vamos fazer isso.

(Imagem 2)
Imagem com figura de computador com duas setas indicando para dois ícones de "SVC" dentro da área de "Cluster". Um deles tem a legenda "svc-portal-noticias" e "NodePort", e está ligado ao pod de "portal notícias". O segundo ícone de "SVC" de legenda "svc-sistema-noiticas" e "NodePort" se liga ao pod de "sistema-noticas".

[00:54] Eu vou abrir o nosso Visual Studio Code mais uma vez e nós vamos criar o nosso 'sistema-noticias.yaml”, o arquivo de declaração dele.

[00:54] Vamos achar “apiVersion: v1”, o tipo que nós queremos criar é um kind: Pod e no metadata: dele vamos chamar de name: sistema-noticias. Sem nenhum problema, nenhuma mudança.

[01:08] E como mais uma vez, ele também vai ser gerenciado por outro serviço, uma label com app: sistemas-noticias.

[01:18] Nas especificações vamos definir as configurações do container, onde o name: dele vai ser sistema-noticias-container. na image:, ao invés de nós utilizarmos a nossa clássica "aluraCursos/portal-noticias", nós vamos usar o aluracursos/sistema-notícias; a imagem que contém todas as informações do nosso sistema, toda implementação para nós podermos executar.

[01:48] Vamos botar o nosso ports: com o “- containerPorts:80”, que nós estamos deixando claro que a nossa aplicação da aluraCursos/sistema-noticias é executada na porta 80. Como nós temos dois pods diferentes, cada um vai ter o seu respectivo IP, não vai ter nenhum conflito de porta.

[02:07] E por fim, precisamos agora criar o nosso para esse sistema, então: “svc-sistema-noticias.yaml” e vamos lá! Digitamos apiVersion: v1. Nós queremos expor ele para o mundo externo então: kind: Service, com um metadata:, um name: svc-sistema-noticias, com as especificações. O tipo dele nós vimos que vai ser um type: NodePort.

[02:37] Em ports: nós vamos fazer o mapeamento de como nós queremos que ele ouça na parte 80 este serviço e despache também para a porta 80.

[02:46] Mais uma vez, nós não precisamos fazer essa declaração do TargetPort se nós queremos que a entrada seja igual a saída. Por fim, o NodePort: - como nós não podemos, nós estamos acessando o nosso cluster de maneira externa, nós precisamos ter uma maneira única de garantir o que nós estamos acessando.

[03:04] Então como nós já estamos utilizando a porta 30000, nós não podemos utilizá-la de novo, então vamos utilizar a porta 30001.

[03:13] Finalizando, basta nós usarmos o nosso “select” e definirmos que nós queremos gerenciar o app que tem as informações com a label sistema-noticias. Copiando, salvando e nós já conseguimos aplicar. Então, vamos lá!

	kubectl apply -f sistema-noticias.yaml 

	kubectl apply -f \svc-sistema-noticias.yaml

kubectl get pods, estão os dois, já em execução.

[03:40] Se nós voltarmos no nosso navegador e abrirmos agora o nosso localhost:30001... está o nosso sistema de notícias!

[03:50] Olhe que legal! Ele vai ser responsável por todo cadastro de notícias dentro do nosso sistema e nós vamos cadastrar todas essas notícias a partir d, mas nós devemos ter em algum lugar. Inclusive, por isso está reclamando em cima para nós guardarmos a informação dessas notícias - e isso vai ser exatamente um banco de dados.

[04:09] Então nós precisamos também subir um banco que vai ser responsável por guardar as informações da nossa notícias, e esse banco vai se comunicar o nosso sistema.

[04:19] Então por esse vídeo é só. Nós conseguimos subir o nosso sistema - que também é NodePort, mas nós precisamos subir o nosso banco e nós vamos ver como nós vamos fazer isso no próximo vídeo. Eu verei vocês lá. Até mais!
*** Pergunta da Alura 2
Maria é uma engenheira de sistemas e está trabalhando em um projeto para hospedar um aplicativo em um cluster no Google Cloud Platform. Ela precisa criar um serviço que use o balanceador de carga da plataforma, tornando o aplicativo acessível tanto dentro quanto fora do cluster de maneira estável. Nesse contexto, ela deve avaliar qual tipo de serviço é o mais adequado para atender sua necessidade.

Marque a alternativa com o serviço ideal para acessar o pod.
**** LoadBalancer
Alternativa correta!
**** ClusterIP.
Alternativa errada! Com ele, conseguiremos acessar o pod apenas dentro do cluster.
**** NodePort
Alternativa errada! Apesar de atender o primeiro requisito, dessa maneira não usaremos o balanceador de carga do cloud provider.
*** Subindo o banco
AP: ver: "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula5_video3_imagem1.png"
AP: ver arquivos: "svc-db-noticias.yaml" e "db-noticias.yaml" da pasta "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/Aula5/"

[00:00] Agora vai ser o seguinte: nós precisamos, como nós vimos, de alguma maneira criar um banco de dados para que nós possamos nos comunicar com o nosso sistema e guardar as notícias. Precisamos ter uma forma de armazenar as nossas notícias.

[00:13] Então se nós viermos na nossa apresentação, nada mais válido do que nós criarmos um pod e um serviço, para que nós possamos nos comunicar com ele.

Imagem com figura de computador conectado por duas setas a dois ícones de "SVC" dentro da área tracejada de Cluster. O primeiro é "svc-portal-noticias" de "NodePort" e se conecta ao pod "portal-noticias". O segundo é "svc-sistema-noticias" de "NodePort" e se conecta ao pod "sistema-noticas", que por sua vez se conecta so "svc-db-noticas" de "ClusterIP", que se liga ao pod "db-noticias".

[00:21] E para isso, como nós queremos comunicação apenas dentro do cluster. Nós não queremos que o nosso banco seja acessível para o mundo externo, nós podemos criar um "ClusterIP” para ele. Nada de misterioso.

[00:33] Então vamos colocar a mão na massa, vamos criar o nosso “db-notícias.yaml”, vamos definir a nossa apIVersion: v1, o kind: Pod e no metadata: vamos colocar um name: db-noticias. Como ele vai ser gerenciado por um serviço, precisamos de uma *label” que vai ser app; db-noticias.

[00:57] Nas especificações nós vamos botar sobre o nosso container, que vai ter um nome de db-noticias-container. Ele vai utilizar a imagem da aluracursos/mysql-db:1. Não vai ser nenhuma das outras imagens, vai ser uma imagem já prontinha com o nosso banco para nós podermos utilizar.

[01:21] E o nosso “ports:”? O que nós vamos definir no nosso containerPort:? Nós vamos falar para ele, e para todo mundo que vê esse arquivo, que o container dessa aplicação do MySQL por padrão é executado na 3306. Pod, a princípio, bem definido.

[01:39] Vamos definir o nosso serviço, como “svc-db-noticias.yaml” e vamos digitar apIVersion: v1, kind: Service e em metadata: vai ser “name:” - e definimos o que ? O nosso svc-db-noticias e em spec: definimos o tipo - que não vai ser o NodePort, e sim um ClusterIP.

[02:08] Em ports: nós vamos definir que nós queremos que as requisições dentro do cluster cheguem neste IP do nosso serviço na porta 3306 e saiam também na 3306. Por fim, basta nós selecionarmos o que nós vamos gerenciar, então: app: db-noticias. Nós salvamos.

[02:37] Nós vamos no nosso banco de dados, no nosso PowerShell e digitamos kubectl apply –f e passamos o nosso .\db-noticias.yaml. Ele foi criado e kubectl apply –f, nosso sistema também “.\svc-db-noticias.yaml” devidamente criado.

	kubectl apply -f db-noticias.yaml

	kubectl apply -f svc-db-noticias.yaml

[02:56] Se nós viermos e vermos o serviço, está sem nenhum problema em execução no nosso clusterIP. Se nós viermos agora no nosso 

	kubectl get pods

o que nós vamos ver? Que tem um erro na linha do db-noticias, olhe que legal!

[03:12] Por quê? Vamos descobrir, vamos executar 

	kubectl describe pods db-noticias

Ele baixou, atribuiu com sucesso ao nó, baixou a imagem - no caso nós tínhamos encontrado - criou o container e inicializou o nosso container.

[03:37] Só que, o que aconteceu? Ele ficou reiniciando indefinidamente. Por quê? Vamos descobrir. Vamos olhar na documentação também do MySQL no Docker Hub. Se nós viermos olhando com bastante paciência, *nós descobrimos que essa parte de variáveis de ambientes precisa ser definida, porque nós precisamos informar diversas informações fim das contas.*

[04:00] Como, por exemplo: qual é a senha do banco que nós estamos criando, qual é o nome do banco, qual é a senha de root, dentre outras coisas. Nós precisamos explicitar essas informações.

[04:11] Só que no nosso Visual Studio Code nós não estamos fazendo isso, então *a pergunta que fica é: como nós podemos utilizar variáveis de ambiente com o Kubernetes para definirmos as informações do nosso container?*

[04:25] Isso nós vamos descobrir na próxima aula e eu verei vocês lá. Até mais!
** Definindo variáveis de ambiente
*** Utilizando variáveis de ambiente
AP: ver imagem "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula6_video0_imagem1-TemModoMelhor...verProximaAula.png" que é uma possibilidade de uso, mas que não é a melhor prática.

[00:00] Então,como nós conseguimos fazer o nosso banco funcionar e agora? Porque ele é baseado na imagem do MySQL e então nós precisamos definir para este container algumas informações.

[00:12] E se nós viermos olhar dentro da página do MySQL no Docker Hub, nós vamos encontrar que nós precisamos obrigatoriamente definir essa variável chamada MySQL_Root_Password, onde ela vai ser a nossa senha de root.

[00:25] Nós também temos opcionalmente a possibilidade de definir qual vai ser o nosso banco, a nossa senha sem ser de root. Se o nosso banco permite senha vazia, ou não - mas também é opcional - qual vai ser. Todas essas informações que nós vamos utilizar na inicialização do nosso banco.

[00:45] Então nós precisamos ter alguma maneira de que no nosso arquivo de definição, para colocarmos essas informações para o nosso container dentro do nosso pod.

[00:54] E como nós fazemos isso? Todas essas informações para um container nós estamos definindo dessa maneira: o nome, a imagem, as portas que nós estamos documentando que estão expostas.

[01:06] Então nada mais válido do que embaixo nós também definirmos as env (Environment variables), que nós vamos definir - e é bem fácil, é bem simples, sem muito mistério. Nós conseguimos agora colocar o - name: para essa variável, que no caso nós vamos definir primeiro a ”MYSQL_ROOT_PASSWORD”, que é obrigatório; e um valor para ela, que no caso do nosso banco vai ser value: “q1w2e3r4”.

[01:33] E nós precisamos no nosso caso específico, não é obrigatório para subir uma imagem do MySQL, mas nós vamos fazer também a definição do MySQL_Password e do MySQL_Database.

[01:47] E como nós podemos definir múltiplas variáveis de ambientes? Será que nós precisamos repetir tudo isso d? Na verdade, não; basta dentro ainda de env:, nós alinharmos outro - name:, que vai entender com esse travessão que nós estamos começando uma nova definição, de uma nova variável e nós colocamos o nome dela que vai ser name: “MYSQL_DATABASE” e o seu respectivo valor que vai ser value: “empresa”, o nome do banco que nós vamos trabalhar.

[02:16] E por fim, a última variável que nós vamos definir vai ser também o nosso MySQL_Password. Vamos colocar ele : - name: “MYSQL_PASSWORD” e o nosso valor vai ser o mesmo do nosso root(value: “q1w2e3r4”).

[02:34] Com isso feito, basta nós voltarmos no nosso PowerShell e deletarmos esse pod atual do nosso db-noticias. A partir daí nós vamos reiniciar e recriar este pod manualmente.

[02:50] Para isso, é só nós utilizarmos o comando que nós viemos trabalhando desde sempre, que é o kubectl apply, e passarmos o nosso arquivo de definição do banco. Ele vai ser criado e se nós digitarmos um kubectl get pods, olhe que legal: ele está com status de “running”.

[03:06] Mas como nós podemos verificar agora se está tudo funcionando direito? Vamos executar esse pod em modo interativo do nosso db-noticias e acessar o banco diretamente dentro dele. Para isso, nós acessamos ele com bash e executamos o nosso“mysql -u root –p, colocamos a nossa senha q1w2e3r4 e o banco está rodando. Se nós digitarmos um show database temos já o nosso banco “empresa”!

[03:37] E se nós usarmos esse banco, nós também conseguimos ver todas as configurações de tabela. Já está o show tables. Nós conseguimos selecionar o usuário para nós conseguirmos fazer o nosso login, sem nenhum problema.

[03:50] E só para deixar claro: todas essas configurações de tabelas e de banco já vieram configuradas nessa imagem (mysql-db) para nós não precisarmos nos preocupar com popular o banco.

[04:00] a questão é só o acesso. Então nós fizemos a definição de tudo o que nós queremos utilizar para inicializar o container do nosso banco. E agora, o que nós precisamos fazer? Se nós voltarmos no nosso login da Alura e apertarmos a tecla “F5”, ainda não está funcionando. Mas por que, se o banco já está rodando?

[04:20] Vamos voltar para o nosso PowerShell e digitar um kubectl get pods. Simplesmente porque o nosso sistema de notícias não é evidente para saber onde está o banco, qual o endereço dele e quais são as informações que ele deve usar para acessar o banco.

[04:37] Então se nós dermos uma olhada mais detalhada também dentro desse sistema de notícias, o que nós veremos? Nós veremos que nós temos um arquivo chamado bancodedados.php.

[04:50] Vocês não precisamos ter conhecimento de PHP, não se preocupem. Dando uma olhada dentro desses arquivos nós percebemos que nós precisamos também definir outras variáveis de ambiente para esse pod.

[05:01] Como: qual é o host do banco, qual é o usuário, qual é a senha e qual é o nome do banco que nós queremos utilizar. Então essas informações nós também vamos precisar utilizar uma variável. Nesse caso, quatro variáveis de ambiente para fazer o acesso deste nosso pod do sistema ao banco.

[05:20] Só que, se nós voltarmos no nosso arquivo, o que nós temos? Observando de maneira um pouco mais crítica, nós temos as configurações do nosso pod, toda a definição dele. Mas nós também temos a definição de ambiente, de variáveis de ambiente e de configuração.

[05:39] Então se nós formos um pouco mais detalhistas, nós vamos ver que nós estamos misturando arquivos de configuração, trechos de configuração com o nosso conteúdo de imagem.

[05:51] Nós estamos deixando tudo muito acoplado. Seria interessante se nós separássemos isso para mantermos as responsabilidades - onde todo esse trecho vai ser responsável pela definição do pod e da imagem que vai ser utilizada para ele.

[06:07] E nas envs nós poderíamos separar isso de alguma maneira para que seja só as partes de configuração para deixar este pod o máximo de portável possível. Nós não estamos atrelando ele à nenhuma configuração específica.

[06:21] Então no próximo vídeo nós vamos entender como nós podemos tornar esse pod mais portável separando, desacoplando informações de configuração da definição do nosso pod - mas isso nós vamos ver no próximo vídeo e eu verei vocês lá. Até mais!

*** Criando um ConfigMap
AP: ver imagens: "aula6_video1_imagem1.png" e "aula6_video1_imagem2.png" na pasta "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/"
AP: ver arquivo: "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/Aula6/db-configmap.yaml"

[00:00] Então, como nós podemos extrair essas informações de configuração para fora do nosso arquivo de definição do nosso bando de dados? Como nós podemos tornar o nosso pod nesse sentido de ser mais portável, para nós não acoplarmos as configurações com a definição do nosso pod?

[00:17] Como eu falei para vocês, o kubernetes vai muito além de ser um simples orquestrador de containers e ele já nos provê diversas soluções nativas para diversos problemas. Para esse caso não seria diferente.

[00:30] *Nós temos a solução chamada ConfigMap, onde ele vai ser responsável por armazenar essas configurações que nós precisamos utilizar dentro de determinados pods, determinados recursos. Nós podemos guardar dentro deles para não acoplarmos o nosso recurso com informações de configuração, por isso um ConfigMap.*

[00:50] E ele vai muito além, nós vamos extrair todo esse trecho que nós definimos no nosso banco de dados para dentro de um ConfigMap. Nós vamos aprender como criar ele também. Mas ele também vai muito além disso, porque ele permite a reutilização e o desacoplamento.

[01:06] Então, a partir de determinado momento nós conseguimos reutilizar configurações definidas dentro de ConfigMaps em diferentes pods. Nós podemos ter pods utilizando diferentes ConfigMaps.

(Imagem 2)
Três ícones de "pod" conectados a um ícone de "cm". O terceiro "pod" também se liga a um segundo ícone de "cm".

[01:18] Então isso nos dá um poder de desacoplamento muito grande e de reutilização também. Mas como é que nós criamos um ConfigMap? É bem fácil! Vocês viram como nós criamos um pod e um serviço até então, mas criar um ConfigMap é tão fácil quanto.

[01:37] Vamos voltar no nosso Visual Studio Code e vamos criar um novo arquivo chamado “db-configmap.yaml” e dentro dele nós vamos definir uma apIVersion: v1 e o kind: ConfigMap.

[01:58] No metadata: nós vamos definir um name: db-configmap e nós vamos definir também agora a data:, o conteúdo dele. Não temos um spec como nós tínhamos tradicionalmente com os nossos outros recursos.

[02:16] dentro nós vamos fazer agora a definição de chave e valor, como nós já vínhamos fazendo antes. Então, vamos só recortar isso e colocar. Vamos fazer a seguinte mudança: nós não vamos definir um env, não vamos definir um name também. O que nós vamos fazer vai ser definir, nesse caso, chaves e valores.

[02:40] Então nós vamos definir um MySQLRoot Password, onde value: “q1w2e3r4”. Colocamos isso sem nenhum problema e podemos até colocar fora das nossas aspas e vai ser a mesmíssima ideia para os outros campos que nós já temos. Vai ter um name: “MySQL_DATABASE” e colocamos também um valor para ele, que vai ser value: “empresa”.

[03:09] Repare em como nós estamos fazendo. Nós estamos definindo todos os nossos campos, todas as nossas chaves e os valores que nós queremos para este ConfigMap. A partir desse momento, quando nós criarmos ele nós vamos conseguir reutilizar tudo sem nenhum problema.a

[03:25] Então definimos o nosso MySQL_ROOT_PASSWORD, o nosso MySQL_DATABASE e o nosso MySQL_PASSWORD, e salvamos o arquivo. Vamos salvar também a operação que nós fizemos no nosso db-noticias.

[03:36] E agora, se nós viermos no nosso PowerShell... Deixe-me eu sair do nosso pod e dar um clear para nós visualizarmos melhor. Basta nós executarmos um kubectl apply –f e passar o nosso .\db-configmap.yaml e foi criado. Simples assim!

	kubectl apply -f db-configmap.yaml

[03:54] Se nós dermos um kubectl get configmap, temos o nosso db-configmap criado há 8 segundos. Nós podemos também descrever ele com o comando kubectl describe configmap db-configmap.

	kubectl get configmap

[04:11] E nós vamos ter todas as informações que nós queremos, o nosso MySQL_DATABASE, o nosso MySQL_PASSWORD e ele sempre fazendo ; a chave e o valor, a chave e o valor; a chave e o valor; a chave e o valor.

	 kubectl describe configmap db-configmap

(Com o comando acima são listadas todas as variávies do db-configmap com seus respectivos valores)

[04:25] Olhe que simples e fácil! A questão agora vai ser como nós utilizamos esse configmap para configurarmos e utilizarmos o nosso banco no nosso projeto.

[04:36] Isso nós vamos descobrir no próximo vídeo, onde nós vamos aplicar este e também criaremos outro configmap para o resto do nosso sistema. Eu vejo vocês lá. Até mais!
*** Aplicando ConfigMap ao projeto
AP: ver imagem: "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula6_video3_imagem1.png" +  "./Kubernetes: Pods, Services e ConfigMaps/Imagens-k8s/aula6_video3_imagem2.png" (A última mostra como seria importarmos uma ou mias variaveis do ConfigMap isoladamente)
AP: ver arquivos na pasta: "./Kubernetes: Pods, Services e ConfigMaps/Arquivos-k8s/aula6/". A ideia é rodar todos os arquivos que estão nessa pasta (não tenho certeza se precisa subir os services (svc-*) - se não precisar: apagá-los da pasta. Depois entrar em http://192.168.59.100:30001/ (obs: atualizar IP) e cadastrar notícias desse sistema-noticia (user/senha=admin/admin). Após isso entrar em http://192.168.59.100:30000 para visualizar as notícias no nosso portal-noticia. Com isso se testa o banco de dados.

AP: Para essa aula eu precisei inserir uma variável de ambiente por conta própria em portal-configmap.yaml, que foi a:
  URL_NOTICIAS: http://192.168.59.100:30001
esse é meu IP atual, rodando outra hora vou precisar atualizá-lo com o que eu obtenho via: kubectl get nodes -o wide
Justifico porque inseri-la: pois o sistema buscava as notícias que eu já havia cadastrado em: http://localhost:30001/noticias.php
o que como veremos abaixo o Linux não entende esse "localhost", mas sim o IP apenas.


----------------------
[00:00] Nós já temos o nosso ConfigMap em execução, mas nós precisamos agora de uma maneira de importarmos esses valores (MYSQL_ROOT_PASSWORD: q1w2e3r4; MYSQL_DATABASE: empresa; MYSQL_PASSWORD: q1w2e3r4) para dentro do container do nosso pod.

AP: Esses trechos (de 00:11 à 01:22)  que veem logo abaixo se referem a configuração via maneira mais verbosa - ver imagem: aula6_video3_imagem2.png
Não é o melhor jeito, após ele mostrar como abaixo dessa maneira, depois ele faz usando configmap:
[00:11] E a declaração vai ser bem parecida de como nós já tínhamos antes. Nós temos o nosso env: - e qual é a variável que nós queremos criar? Uma variável chamada MYSQL_ROOT_PASSWORD, mas agora nós não vamos simplesmente definir um value para ela, nós vamos definir de onde ela vem.

[00:28] Então vai ser valueFrom: e nós vamos informar a origem dela - que vem de um configMap”, que tem uma referência a uma chave, entãoKeyRef. O nome desse configMapKeyRef é "db-configmap" e a chave que nós queremos colocar dentro dessa nossa variável é exatamente essa, de MYSQL_ROOT_PASSWORD.

[00:54] Então, MYSQL_ROOT_PASSWORD, nós estamos fazendo o acesso d (db_noticias.yaml) (db-configmap.yaml)e estamos armazenando (MYSQL_ROOT_PASSWORD), certo?

[01:08] E se nós quiséssemos fazer isso para MYSQL_DATABASE e para MySQL_PASSWORD também, nós teremos que repetir toda essa declaração mais duas vezes. Então nosso arquivo, por mais que ficasse portável, ele ficaria bem grande também.

[01:22] Como nesse caso nós queremos fazer declaração de todas as variáveis que estão dentro do nosso configMap, nós podemos fazer uma declaração mais simples e ao invés de importar uma a uma, nós podemos importar todo o nosso configMap de uma única vez.



[01:39] Como? Nós podemos fazer a referência ao invés de variável à variável, nós podemos fazer referência direto ao configMap. Então,configMapRef e qual é o nome desse configMap? É db-configmap! Salvamos ele .

[01:55] E agora nós já temos ele em execução, então nós vamos fazer ele parar para ele usar o configMap efetivamente, kubectl delete pod db-noticias. E nós vamos aplicar novamente com essa nova declaração que nós estamos fazendo ao nosso configMap.

[02:13] Então, kubectl apply -f .\db-noticias.yaml, se nós digitarmos um kubectl get pods e agora, ele está em execução.

[02:21] Mais uma vez, confirmando: 

	kubectl exec -it db-noticias -- bash 

e temos o nosso 
	mysql -u root -p

a nossa senha q1w2e3r4 e no "show databases" está o nosso banco empresa.

[02:43] Só que... O que nós precisamos agora? Se nós voltarmos no nosso navegador, no nosso sistema, e apertarmos a tecla “F5”, nós precisamos ainda fazer a referência a esse banco - que era o que nós já estávamos planejando.

[02:57] Se nós acessarmos o nosso sistema com comando também kubectl exec –it no nosso sistema-noticias, nós conseguimos ver o que ? Se ele estava nos arquivos nós temos esse arquivo chamado “bancodedados.php”.

[03:14] Que nós não precisamos nos preocupar com o “php”, não vai ter nenhum foco em PHP nesse curso, fique tranquilo. Mas nós olhando esse arquivo nós conseguimos ver que precisamos declarar essas quatro variáveis, para que ele consiga localizar o banco.

[03:27] Qual é o host, o endereço desse banco, qual é o usuário dele, a senha e o nome do banco que nós queremos utilizar - para nós fazermos isso é bem simples

[03:38] Então, o que nós vamos fazer agora? Nós vamos simplesmente criar um novo configMap, só que dessa vez para o nosso sistema.

[03:45] Então vamos ter agora o nosso sistema-configmap.yaml. Dentro dele, a versão da API vai continuar sendo a v1, o kind: ConfigMap, no nosso metadata: vamos definir um name: sistema-configmap.

[04:11] E por fim, no nosso "data:" nós vamos definir essas quatro variáveis. O nosso “HOST-DB” vai ter um valor e como nós queremos fazer a comunicação do nosso serviço, do nosso pod de sistema com o serviço do nosso banco de dados.

(Imagem 1)
Imagem com figura de computador conectado por duas setas a dois ícones de "SVC" dentro da área tracejada de Cluster. O primeiro é "svc-portal-noticias" de "NodePort" e se conecta ao pod "portal-noticias". O segundo é "svc-sistema-noticias" de "NodePort" e se conecta ao pod "sistema-noticas", que por sua vez se conecta so "svc-db-noticas" de "ClusterIP", que se liga ao pod "db-noticias".

[04:30] O que nós precisamos? Eu vou abrir uma nova aba do PowerShell e digitar kubectl get svc. Nós precisamos fazer a referência ou ao nosso DNS, que é o nosso nome do nosso serviço para nós acessarmos o nosso pod do banco, ou ao IP também.

[04:48] Ambos estão ouvindo na porta 3306, então vamos fazer isso . Vamos colocar que o nosso “HOST_DB”, nada mais é do que o nosso próprio DNS. Vamos utilizar ele para mostrar que funciona também. Vamos copiar e vamos colocar ele na porta 3306.

[05:09] Vamos definir o nosso USER_DB, que é “root”, o nosso PASS_DB que é q1w2e3r4 e por fim o nosso DATABASE_DB, que é o "empresa".

[05:25] Basta nós voltarmos agora. Vamos sair de dentro desse container, desse pod, vamos digitar kubectl apply –f no nosso .\sistema-configmap.yaml. E ele foi criado.

[05:36] E agora nós precisamos no nosso sistema de notícias fazer a mesma coisa que nós fizemos antes, importar este configMap para uso, ou seja, envFrom: configmapRef e o nome dele, que é o nosso sistema-configmap.

[05:54] Vamos precisar agora deletar e recriar ele: delete pod sistema-notícias e vamos aplicar novamente kubectl apply -f .\sistema-noticias.yaml. Foi criado.

[06:08] E se nós voltarmos agora e apertarmos a tecla “F5”, o erro some. Então agora nós já conseguimos fazer o login, que nós vimos no banco que é (usuário) “admin” e (senha) “admin”. Podemos vir, digitarmos e estaremos autenticados. Podemos cadastrar as notícias.

[06:24] Então, podemos vir em “Nova Notícia” e (cadastrarmos) colocarmos uma notícia com alguma informação qualquer e colocarmos também uma foto, onde podemos colocar uma foto qualquer da Alura. Vamos salvar.

[06:38] Repare que ele salvou e agora no nosso portal nós queremos que esse portal se comunique com esse sistema para fazer a exibição dessa notícia para nós. Se nós queremos fazer essa comunicação voltando na nossa apresentação, nós queremos também fazer essa comunicação via variável de ambiente.

[06:58] Se nós viermos no nosso PowerShell e entrarmos em modo interativo dentro do nosso portal-noticias, o que nós vamos ver? Vai ser algo bem parecido com o nosso sistema, nós temos um arquivo chamado de “configuracao”, nesse arquivo nós precisamos definir qual é o IP do nosso sistema. Bem simples e prático.

[07:20] Então vamos voltar no nosso Visual Studio Code e vamos criar o nosso também “portal-configmap.yaml” e dentro dele vamos definir também todas as mesmas coisas que nós já viemos fazendo com os configMaps.

[07:36] Então, configMap, vamos definir um metadata: para ele, que vai ser um name: portal-configmap. Vamos definir a data:, que vai ser "IP_SISTEMA:".

[07:50] E nesse momento, o que vai acontecer? Quem está utilizando o Windows assim como eu, vai colocar o "localhost". E o nosso sistema está sendo executado em qual porta? localhost:30001, onde nós definimos o nosso NodePort.

[08:09] Então quem está utilizando Windows vai colocar “http://localhost”. Atentem-se ao “http://”, ele é necessário. Na porta 30001, caso você esteja utilizando o Linux é aquela velha história: você vai precisar colocar o seu INTERNAL_IP (AP: que é pego com kubectl get nodes -o wide. Na hora de passar o URL para o navegador: digitar "IP:PORTA"). Então, caso seja 192.168.99.106, você vai colocar também 192.168.99.106, mas como eu estou no Windows vou deixar o localhost.

[08:41] E agora nós vamos fazer o que ? Nós vamos simplesmente aplicar este arquivo também ao nosso cluster, kubectl apply -f .\portal-configmap.yaml e vamos utilizar esse configMap dentro do nosso portal de notícias.

[08:57] Com o nosso envFrom e vamos colocar mais uma vez o nosso configMapRef, onde o nome do configMap que nós queremos fazer referência é o nosso portal-configmap.

[09:12] Vamos agora voltar, sair do nosso container dentro do nosso pod e recriar esse pod no nosso portal de notícias e vamos reaplicar com essa devida mudança.

[09:26] Então vai ser o nosso kubectl apply –f em cima do nosso novo arquivo. Então, kubectl apply -f .\portal-noticias.yaml.

[09:55] E vamos aplicar. Se nós voltarmos agora no nosso navegador, vamos colocar, apertar a tecla “F5” - e olhe só, está a nossa notícia com uma imagem e a nossa informação que nós definimos.

[10:09] Comunicamos os três pods via serviço, utilizando as melhores práticas, um NodePort para os dois que precisam ser NodePort e um ClusterIP, para o que é o ClusterIP.

[10:20] E também, se nós voltarmos na nossa aplicação, para visualizarmos tudo o que foi feito, nós também utilizamos as variáveis de ambiente para mantermos essa comunicação agnóstica. Ali nós vamos definir onde eles estão.
*** Conclusão
No último vídeo, definimos como variável de ambiente o endereço do sistema de notícias para o nosso portal de notícias conseguir acessá-lo. Fizemos isso utilizando o IP do node seguido da porta exposta pelo nosso NodePort, nesse caso localhost:30001 (AP: para acessar no linux é preciso colocar no navegador "IP:PORTA", e não "localhost:PORTA").

Caso tivéssemos múltiplos nodes em nosso cluster, tudo funcionaria da mesma maneira, pois as portas mapeadas pelo NodePort são compartilhadas entre os IP's de todos os nodes.

Mais informações podem ser adquiridas em https://kubernetes.io/docs/concepts/services-networking/service/#nodeport.

* Curso de Kubernetes: Deployments, Volumes e Escalabilidade
https://cursos.alura.com.br/course/kubernetes-deployments-volumes-escalabilidade
** Aula 01 - Conhecendo ReplicaSets e Deployments
*** Conhecendo ReplicaSets
AP: Ver exemplo em: "./Arquivos-k8s/kubernetes-parte2-Aula1/exemplo-replicaset.yaml" (obs: depois eu consegui o arquivo da aula, é o "portal-noticias-replicaset.yaml")
AP: Ver também: "./Imagens/Aula01_video01_img01.png" à "Aula01_video01_img05.png"

[00:54] Nós vamos começar falando sobre os ReplicaSets e depois vamos falar sobre os Deployments.

[01:00] Para entendermos o que é um ReplicaSet é bem fácil. Vamos visualizar este Pod. Nós acabamos de ver que, caso ele falhe, pare de ser executado, seja interrompido, ele morreu para sempre. Ele nunca mais vai voltar, não existe nenhuma maneira de fazer ele voltar.

[01:18] E a pergunta que queremos responder é como conseguimos criar outro para assumir o lugar dele, de maneira automática.

[01:25] Como assim? Vamos visualizar. Temos os nossos três Pods do banco de dado de notícias, do portal de notícias e do sistema de notícias. Caso, nós pararmos o nosso Pod do portal de notícias, sem nenhum mistério, simulando uma interrupção, o que vai acontecer?

[01:44] A nossa aplicação do portal não está mais disponível porque acabamos de interromper ela, mas, vamos visualizar só para ter a garantia.

[01:54] Ou seja, quando nosso Pod foi interrompido, por algum motivo, o que aconteceu? Aconteceu que ele não voltou por nenhuma razão. E nós queremos contornar esse tipo de situação, que em caso de falha ele volte.

[02:08] Então, voltando à nossa apresentação, nós temos uma estrutura que resolve isso para nós, que são os ReplicaSets.

[02:18] Então, vimos que um Pod é uma estrutura que encapsula um ou mais containers e *um ReplicaSet nada mais é que uma estrutura que encapsula, pode encapsular, na verdade, um ou mais Pods.*

[02:34] Repara nas duas coisas que acabei de falar. Ela tem a capacidade de encapsular um ou mais Pods, ou seja, nós não temos a obrigatoriedade de criar um Pod com um ReplicaSet. Não é a toa que nós viemos fazendo isso, até agora, no curso. E que ele também pode gerenciar um ou mais Pods, ao mesmo tempo.

[02:55] Logo, *se, um desses Pods falhar, e ele nunca mais vai voltar, mas o ReplicaSet vai conseguir criar um novo para nós. E isso também vai funcionar para diversos Pods gerenciados por um mesmo ReplicaSet.*

[03:09] Então, o que vai acontecer? Caso algum desses Pods falhe, o que vai acontecer? Nós vamos ter um número de Pods desejados diferentes do número de Pods prontos. Logo, o próprio ReplicaSet vai criar um Pod novo para substituir esse que parou de funcionar.

[03:30] E por que nós podemos ter diversas réplicas? Por diversos motivos que ainda vamos abordar no decorrer do curso.

[03:35] Mas, um deles é que enquanto este Pod está voltando a ser executado, para que consigamos acessar nossa aplicação, nós temos outras três formas de acessar esta mesma aplicação, porque nós possuímos cópias dela em execução, então, enquanto essa aqui não volta, nós temos outras três para receber as requisições do nosso usuário.

[03:56] *E, também, caso nós estejamos recebendo muita requisição, nós conseguimos dividir através de um Set que vai fazer o balanceamento de carga, as requisições para esses Pods de maneira bem igual.*

[04:11] Então, vamos colocar a mão na massa. Vamos criar o nosso primeiro ReplicaSet. É bem fácil.

[04:16] O que nós vamos fazer? Nós vamos criar um novo arquivo, clicando no botão "New File" no canto superior esquerdo da tela, chamado portal-noticias-replicaset.yaml (AP: não tinha esse arquivo no git do curso - eu deixei um modelo de exemplo em "./Arquivos-k8s/kubernetes-parte2-Aula1/exemplo-replicaset.yaml"). E, dentro desse arquivo, *a primeira grande diferença que nós vamos ver é que a versão da api não vai ser só v1, vai ser um grupo específico da v1 que é a apps/v1,* que podemos até conferir na própria documentação do Kubernetes.

[04:45] Então, sobre ReplicaSets, nós conseguimos ver que apiVersion: apps/v1. O tipo que nós queremos criar é um ReplicaSet e ele também possui um metadata, então, qual vai ser o nome desse ReplicaSet? Vai ser portal-noticias-replicaset.

[05:08] E uma especificação para ele, o que vai conter neste ReplicaSet. É nesse momento, em que talvez você esteja pensando, basta copiar o trecho das especificações do nosso portal-noticias e colar ele aqui no ReplicaSet e está tudo feito.

[05:23] Não é bem assim. Nós vamos ter algumas peculiaridades, então, eu já vou comentar isso para termos como base o que nós vamos utilizar e vamos fazer o seguinte: nós precisamos de início, definir um template.

[05:38] Qual vai ser o conteúdo, qual vai ser o modelo deste ReplicaSet, qual vai ser o padrão dele? Então, nós vamos definir o que no portal-noticias-replicaset? As informações do nosso Pod.

[05:50] Então, o nosso Pod possui o que? Ele possui um metadata e esse metadata tem um name, como já está especificado mais abaixo do trecho copiado. Vou, até, descomentar este trecho e copiar.

[06:02] Nós temos o metada que contém um name, um label que é app: portal-noticias, então, já definimos o metadata dos Pods, ou do Pod, no singular, que vai ser gerenciado por este ReplicaSet.

[06:19] Esse metadata (name: portal-noticias) é do Pod de dentro do ReplicaSet e este metadata (name: portal-noticias-replicaset) é do nosso ReplicaSet. O trecho copiado anteriormente já pode ser apagado, inclusive.

[06:30] E, agora, precisamos definir o que? Quais são as especificações do nosso Pod. Então, basta pegarmos o mesmo trecho do arquivo portal-noticias.yaml e colocar alinhado com o nosso metadata.

[06:47] Nós temos todas as informações do nosso Pod, que vai ser gerenciado pelo nosso ReplicaSet. Então, dentro de template nós vamos definir todas as informações do nosso Pod.

[07:03] Agora falta o que? Falta definirmos, alinhado com template, o número de réplicas que queremos para nossa aplicação, então, como falei para vocês nós podemos ter um ou mais Pods sendo gerenciados por um ReplicaSet.

[07:22] Nós podemos definir, por padrão, se nós não informarmos esse número vai ser 1, mas, podemos colocar 1, 2, 3, 4 e podemos colocar o quanto quisermos neste cenário. Então, eu vou colocar que nós queremos três réplicas, para caso alguma delas falhe, nós ainda vamos ter duas de backup para receber as requisições do nosso usuário.

[07:47] E, por fim, falta definirmos mais uma coisa que parece ser um pouco redundante, que acaba sendo, visualmente falando. Que por mais que todas essas informações estejam definidas dentro das especificações deste ReplicaSet (metadata: name: portal-noticias-replicaset), o Kubernetes não sabe que este ReplicaSet deve gerenciar este template, que definimos anteriormente.

[08:11] Nós precisamos fazer o seguinte, informar que o nosso ReplicaSet, assim como um serviço, vai selecionar os recursos que tenham um matchLabels que encaixem, que batam o valor das labels com app: portal-noticias, então, esse matchLabels tem que ser igual, idêntico ao labels.

[08:38] Senão, não vai funcionar. No momento que tentarmos aplicar esse arquivo de definição, ele não vai funcionar. Então, os matchLabels e labels, repara, sempre serão iguais.

[08:51] Se salvarmos ele e viermos no nosso Powershell e executar o nosso kubectl apply -f .\portal-noticias-replicaset.yaml, o que vai acontecer? Ele foi criado.

[09:06] E se nós usarmos o comando kubectl get pods, nós temos três Pods, como nós definimos no nosso número de réplicas, para o nosso portal de notícias. E cada um desses Pods segue a mesma receita.

[09:20] Não é à toa que todos começam o nome com portal-noticias-replicaset traço um identificador único (AP: exemplo: "portal-noticias-replicaset-dkppr"). *E todos eles vão estar expondo na porta 80, como já definimos.*

[09:32] Todos vão utilizar as mesmas variáveis do nosso portal do configmap sem nenhuma mudança. os três são iguais, só os identificadores que são diferentes.

[09:43] Não é à toa que se viermos no nosso navegador, clicando no ícone "Internet" na barra de ferramentas, e tentar acessar o nosso portal temos tudo funcionando.

[10:16] Se tentarmos forçar a parada de um desses Pods com o comando kubectl delete pod e colocar um deles para ser removido, o que vai acontecer? Vamos colocar no comando o Pod portal-noticias-replicaset-dkppr, vamos remover e vamos confirmar se, realmente, temos dois ou três Pods em execução.

[10:39] kubectl get pods, repara que agora temos um novo Pod no lugar daquele, com identificador diferente, porém, com o mesmo comportamento, sem nenhum mistério.

[10:53] Se nós executarmos um outro comando que é o kubectl get rs ou replicasets o que vai acontecer: Ele vai nos mostrar que o número desejado de Pods, atualmente, por este ReplicaSet é três, que nós definimos no nosso número de réplicas, e que o nosso atual de prontos, que nós temos, também é três. Então, tudo vai correr sem nenhum problema.

[11:19] Para vocês verem mais um cenário eu vou abrir outro Powershell e nós vamos fazer o seguinte. Eu vou dividir a tela entre os dois e vou limpar a tela e na da esquerda eu vou comandar kubectl get replicaset --watch e do outro lado eu vou comandar kubectl get pods e vou dar um kubectl delete pods em qualquer um dos três Pods que está sendo gerenciado pelo nosso ReplicaSet.
AP: ex:
lucas@lucas:~$ kubectl get replicaset --watch
NAME               DESIRED   CURRENT   READY   AGE
nginx-replicaset   6         0         0       0s
nginx-replicaset   6         0         0       0s
nginx-replicaset   6         6         0       0s
nginx-replicaset   6         6         1       13s
nginx-replicaset   6         6         2       15s
nginx-replicaset   6         6         3       17s
nginx-replicaset   6         6         4       19s
nginx-replicaset   6         6         5       20s
nginx-replicaset   6         6         6       23s


[11:48] Vou colocar este Pod no comando kubectl delete pods, vou colar e vou parar. O que aconteceu... por um momento tivemos dois, porque um foi removido, foi interrompido, depois nós tivemos três, mas, ele ainda não estava pronto, e, por fim, nós ficamos no nosso estado desejado, sem nenhum mistério.

[12:07] E ele foi criado um novo, como já vimos. Se utilizarmos o comando kubectl get pods ele vai ter criado um novo, em relação ao anterior, que nesse caso é o portal-noticias-replicaset-mzskh.

[12:21] Então, utilizando ReplicaSets nós conseguimos definir que a nossa aplicação deve estar sempre disponível para o usuário, o Kubernetes tem que manter ela sempre disponível em caso de falhas ou interrupções.

[12:33] E também, definir o número de réplicas para o nosso usuário conseguir acessar, caso uma dê problema e ainda não tenha voltado. Ou dividir, fazer todo o balanceamento de carga.

[12:43] Então, por mais que agora, só para finalizarmos, o nosso Pod esteja dentro de um ReplicaSet, o nosso serviço continua funcionando sem nenhuma mudança porque ele seleciona por qualquer um que tenha a chave app com valor portal-noticias (AP: no meu arquivo de exemplo citado no começo do tópico: "nginx-app").

[13:00] E ele continua tendo isso porque nós definimos no labels do arquivo portal-noticias-replicaset.yaml, então, não muda ele estar dentro de um ReplicaSet. Vai tudo funcionar da mesma maneira.

[13:11] Agora, o nosso serviço, o que ele faz? Para a gente finalizar, vamos voltar na nossa apresentação e ver um trecho que explicita o que está acontecendo agora.

[13:36] O nosso serviço, agora, faz todo esse balanceamento sem nenhum mistério, para três Pods gerenciados por um ReplicaSet.

[13:46] No próximo vídeo vamos entender o que são os Deployments, no que eles podem nos ajudar e como eles são diferentes de um ReplicaSet. 
*** Conhecendo Deployments
AP: Ver exemplo em: "./Arquivos-k8s/kubernetes-parte2-Aula1/nginx-deployment.yaml"
AP: Ver também: "./Imagens/Aula01_video02_img01.png"

[00:00] Nós acabamos de ver sobre os ReplicaSets, que permitem a criação de Pods de maneira automática, em caso de falha, todo esse controle do estado desejado e do estado atual. E nós vamos falar agora sobre o segundo recurso que permite a mesma coisa, que são os Deployments.

[00:19] Mas, o que muda na prática? Nós vimos que um ReplicaSet nada mais é do que esse conjunto de réplicas que permite a criação, de maneira automática, em caso de falhas de um Pod, dentro de um cluster gerenciado por um ReplicaSet.

[00:34] *Mas, um Deployment nada mais é do que uma camada acima de um ReplicaSet. Então, quando nós definimos um Deployment, nós estamos, automaticamente, definindo um ReplicaSet, sem nenhum mistério.*

[00:52] Mas, o que muda? Quando criamos um Deployment, nós criamos um ReplicaSet, mas, o que vem de novo com isso? O que ganhamos de adicional? Vamos verificar, vamos criar o nosso primeiro Deployment para entendermos.

[01:04] Para isso, vamos criar um arquivo clicando no ícone de “Novo Arquivo” no canto superior esquerdo da tela e adicionar uma nova pasta na lista, que vai ser o nosso nginx-deployment.yaml e a definição dele vai ser idêntica a de um ReplicaSet. A versão apiVersion também vai ser apps/v1; vamos colocar o nosso kind, que vai ser um Deployment.

[01:24] No nosso metadata vamos definir um name pra ele, que vamos chamar também de nginx-deployment.

[01:33] Vamos colocar nossas especificações spec, que nós vimos que vai definir, inicialmente, o número de réplicas. Podemos colocar três, novamente, para exemplificar e vamos colocar as informações do nosso template, que vai ter um metadata, que serão as informações dos nossos Pods.

[01:54] Vamos colocar um name: nginx-pod e as labels vamos colocar a chave app com um valor, também, de nginx-pod.

[02:04] Nas especificações spec do nosso Pod vamos colocar as informações dos containers, que teremos um name: nginx-container e uma image que vamos colocar nginx-stable.

[02:21] Por fim, só que *é boa prática vamos colocar o nosso ports: containerPort: 80*. E agora só falta o nosso seletor, que vai ter *o nosso matchLabels, que tem que ser igual ao nosso labels do nosso metadata*, então, app: nginx-pod. Nada de novo, a mesma declaração de um ReplicaSet, só trocamos em cima para Deployment.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      name: nginx-pod
      labels:
        app: nginx-pod #ref 01: Tem que ser igual ao do ref02
    spec:
      containers:
        - name: nginx-container
          image: nginx:stable
          ports:
            - containerPort: 80
  selector:
    matchLabels:
      app: nginx-pod #ref 02: Tem que ser igual ao do ref01

[02:50] Se viermos na tela do Windows PowerShell e executar o comando kubectl apply –f .\nginx-deployment.yaml, ele foi criado. Se executarmos o comando kubectl get pods, temos agora três Pods em execução, por causa deste Deployment.

[03:09] Se dermos um 

	kubectl get rs

temos dois ReplicaSets. O nosso portal-noticias-replicaset (AP: do tópico da aula de cima, no meu caso é o do exemplo-replicaset.yaml) e o nosso recém criado nginx-deployment-6bf9fcdcc7, mas, ele tem um ReplicaSet, porque, como vimos, temos uma super camada, um super conjunto, que é o Deployment e dentro de um Deployment nós criamos, automaticamente, um ReplicaSet que vai gerenciar os Pods.

[03:34] E se formos um pouco mais além, temos o 

	kubectl get deployments

e temos o nosso Deployment criado, o nosso nginx-deployment, que tem os três Pods em execução, sem nenhum problema.

[03:48] O que vai mudar na prática agora? *A grande vantagem do uso de Deployments é que, assim como temos um git, por exemplo, para o nosso controle de versionamento de código, nós temos os Deployments em Kubernetes, que permitem o nosso controle de versionamento das nossas imagens e Pods, olha que legal.*

[04:09] O que muda agora? Conseguimos utilizar comandos extras para validar e verificar todo o nosso fluxo. Então, por exemplo, podemos executar o comando 

	kubectl rollout history deployment nginx-deployment  

e ele nos mostra que temos uma revisão, a que estamos atualmente, um estado atual, e não temos nenhuma causa de mudança, porque ela é nossa primeira, não definimos nada para ela.

AP: saida do meu console:
deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
1         <none>


[04:44] Mas, e se eu quiser mudar aquela versão stable para, por exemplo, a versão latest (mudar dentro do yaml a linha 15)
? O que pode acontecer? Eu posso, simplesmente, agora, executar o comando, e posso utilizar a flag no final, --record e o que vai acontece?

	kubectl apply -f nginx-deployment.yaml --record

kubectl rollout history deployment nginx-deployment 
deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl apply --filename=nginx-deployment.yaml --record=true

Ele vai configurar, vai alterar para nós, se eu der um kubectl get pods, repara, recém-criados e cada um deles vai estar utilizando a versão latest.

[05:20] Mas, se eu executar, de novo, o comando

	kubectl rollout history deployment nginx-deployment

o que vai acontecer? Ele tem, agora, a segunda revisão, a que estamos atualmente. E podemos alterar este "CHANGE-CAUSE" (AP: 2a coluna da tabela da saída do console acima) para ficar mais claro o que fizemos.

	kubectl annotate deploy nginx-deployment kubernetes.io/change-cause="Definindo a imagem com versão latest, por exemplo, e ele vai anotar para nós."

(AP: Saída no meu terminal:)
$ kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
1         <none>
2         Definindo a imagem com versão latest, por exemplo, e ele vai anotar para nós.

[06:18] Se executarmos, novamente, o comando kubectl rollout history deployment xginx-deployment o que vai acontecer é que teremos a nossa primeira revisão, que foi a de criação, e a nossa segunda, que foi Definindo a imagem com versão latest.

[06:29] E podemos ir além. Se eu quiser alterar mais uma vez, por exemplo, para a versão 1 (AP: denovo mudar na linha 15 para: "image: nginx:1") o que vai acontecer? Vamos usar o comando kubectl apply –f .\nginx.deployment.yaml com nosso arquivo de definição com flag --record e o que vai acontecer?

$ kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
2         Definindo a imagem com versão latest, por exemplo, e ele vai anotar para nós.
3         kubectl apply --filename=nginx-deployment.yaml --record=true

(AP: na saída do console acima o meu não veio a REVISION 1, mas na do video do professor veio também a "1 <none>").

[06:46] Temos mais uma causa de mudança, mais uma revisão com o mesmo comando. Ele grava por padrão comando que foi executado para essa alteração.

[06:57] E podemos, mais uma vez, alterar para ficar mais claro. Repetindo o mesmo comando de antes, do nosso annotate, ele vai alterar para nossa revisão atual, que é a três, que nós estamos, então kubectl annotate deployment nginx-deployment, com nosso CHANGE-CAUSE sendo Definindo a imagem com versão 1.

	kubectl annotate deploy nginx-deployment kubernetes.io/change-cause="Definindo a imagem com versão 1"

[07:19] Se dermos agora um kubectl get pods temos tudo em execução, sem nenhum mistério e se dermos um kubectl rollout history mais uma vez, temos aqui todo nosso controle de versionamento. Tudo sendo feito, exatamente, da maneira que esperávamos.

(AP: minha saída no terminal:)
$ kubectl rollout history deployment nginx-deployment
deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
2         Definindo a imagem com versão latest, por exemplo, e ele vai anotar para nós.
3         Definindo a imagem com versão 1


[07:38] Pra validarmos se tudo está da maneira que esperamos, vamos ao comando kubectl describe pod e eu vou descrever qualquer um dos nossos Pods do nginx-deployment, vou colar ele xginx-deployment-5c6d6c6479-9qfnd e podemos ver que nas nossas configurações de imagens, o que está acontecendo: ele está utilizando a versão 1, que foi o que definimos.

[08:00] Mas, e se eu quiser voltar para alguma versão específica, fazer um rollback, alguma alteração no que já tenho, invés de eu ter que alterar manualmente no meu arquivo de definição, por exemplo.

[08:13] O que eu consigo fazer aqui é, simplesmente, mais uma vez executando o comando de history, digamos que eu queira voltar para versão latest, eu consigo executar o comando 

	kubectl rollout undo deployment
(ou: kubectl rollout undo deployment --to-revision=2 )


e passando ou não do nosso Deployment que é nginx-deployment, eu consigo voltar para alguma revisão específica.

deployment.apps/nginx-deployment 
REVISION  CHANGE-CAUSE
3         Definindo a imagem com versão 1
4         Definindo a imagem com versão latest, por exemplo, e ele vai anotar para nós.


[08:36] Então --to-revision=2 eu consigo voltar para versão dois e ele vai fazer tudo isso, magicamente, se eu usar o comando kubectl get pods todos estão sendo criados e *estão encerrando os anteriores e criando os novos*, como, por exemplo, esse nginx-deployment-6c88c4c95c-b7k8g e se eu usar o comando kubectl describe pod, colo este Pod utilizando a versão latest.

[09:05] Então, quando utilizamos nossos Deployments, o que conseguimos fazer? Conseguimos, simplesmente, ter uma camada extra acima de um ReplicaSet, que consegue gerenciar as imagens, todo o versionamento do que estamos definindo, controle de atualização em cima das nossas imagens e Pods. Que legal, não é verdade?

[09:28] *Então, no fim das contas, a boa prática, o mais comum que vocês irão ver quando vocês forem criar Pods é criar eles através de Deployments, que eles já vão permitir todo esse controle de versionamento e também os benefícios de um ReplicaSet.* 

[09:43] Nada impede de criarmos manualmente, como viemos fazendo com Pods e ReplicaSets, mas, o mais comum, a boa prática, é fazer a criação através de Deployments, por conta desses benefícios de controle de versionamento e de estabilidade, disponibilidade da nossa aplicação.

[10:00] Então, com esse vídeo conhecemos o que é um Deployment e no próximo iremos aplicar esses conceitos de Deployment ao nosso projeto de notícias.

*** Aplicando Deployments ao projeto
AP: Ver imagem: "./Imagens/aula1_video4_imagem1.png"
AP: Arquivos em: "./Arquivos-k8s/kubernetes-parte2-Aula1/"
[00:00] O foco desse vídeo vai ser fazer nós alcançarmos este modelo.

(Imagem01)
Imagem com uma figura de um computador originando dois caminhos distintos indicados por setas de duplo sentido: o primeiro conecta a máquina ao ícone de SVC e depois ao Deployment Portal contendo três ícones de Pod, e o segundo caminho se liga ao ícone de SVC, que se liga ao Deployment Sistema contendo um Pod, depois ao SVC novamente e por fim ao Deployment DB com um ícone de pod.

[00:07] Já temos dois serviços que fazem o acesso ao nosso portal, nós vamos transformar ele num Deployment, nós temos um ReplicaSet, atualmente.

[00:14] E vamos transformar o nosso simples Pod do sistema e do nosso banco de dados em Deployment pra termos, *além da questão do reinício automático, em caso de falhas, também teremos a nossa questão do controle de versionamento*, para tudo ficar sem nenhum problema, tudo padronizado, utilizando os melhores recursos possíveis para o nosso momento.

[00:38] Então, bem fácil. Vamos colocar a mão na massa. O primeiro que iremos trabalhar em cima vai ser o nosso portal de notícias.

	kubectl delete pods --all
	(AP: Parar tudo o mais que estiver de pé)
 	
[01:41] Agora, a ideia vai ser criarmos nosso Deployment pro nosso portal. Já temos toda a estrutura do nosso ReplicaSet, então, basta virmos aqui, pegarmos o conteúdo todo do arquivo portal-noticias-replicaset.yaml.

[01:54] Vamos criar um arquivo novo chamado portal-noticias-deployment.yaml, vamos apagar o do nosso ReplicaSet, eu copiei todo o conteúdo e vou colar dentro do nosso Deployment.

[02:10] Vimos que a declaração toda é idêntica, aqui em cima só vou trocar para Deployment e vou colocar para o name dentro do metadata o -deployment. Nada de novo, tudo vai ser igual. Então, se viermos no comando kubectl apply -f e definir o nosso portal-noticias-deployment ele foi devidamente criado.

[02:35] Então, usando o comando kubectl get pods no terminal e estão os três em execução.

[02:39] Se eu usar kubectl rollout history e colocar o nosso Deployment do nosso portal-noticias-deployment o que vai acontecer? Estaremos na nossa revisão 1, sem mudanças.

[02:54] Vamos deixar aqui bem claro, então, vamos praticar um comando kubectl annotate deployment o nosso portal-noticias-deployment e vamos colocar o nosso kubernets.io/change-cause e definir para Criando portal de notícias na versão 1, pra praticarmos os comandos.

[03:29] Se viermos na revisão, o nosso CHANGE-CAUSE todo definido, sem nenhum problema.

(AP: O professor do curso mostrou uma versão do sisteminha de finalização do "Curso de Kubernetes: Pods, Services e ConfigMaps" que ainda estava rodando - ele já tinha cadastrado notícias no banco de dados, e ele mostrou que elas apareciam no navegador quando carregada a página. Na sequência ele finalizou os processos e subiu o sistema denovo, só que agora como tipos deployment's. Ele mostrou que as notícias que havíamos visualizado anteriormente não apareciam mais quando carregado o site. Porquê? É pelo motivo de os pods serem efêmeros. Será preciso aplicar a ferramenta do kubernets de volume, como será visto nas próximas aulas desse curso. Isso aparece nessa aula em [09:12]min)

[03:35] Agora, vamos fazer o seguinte. Temos o nosso kubectl get pods e vamos fazer a mesma coisa para nosso sistema-noticias e para o nosso banco de dados db-noticias.

AP: abaixo foi criado o arquivo sistema-noticias-deployment
[04:02] E, por fim, o que nós vamos fazer? Vamos definir nossa especificação, que já vimos que vamos definir o número de réplicas para um, porque só queremos uma réplica do nosso sistema de notícias em execução. Vamos deixar nossa apresentação e nós criamos só uma réplica do nosso sistema de notícias e temos que definir, agora, o nosso template.

[05:07] Salvamos e se viermos no PowerShell e usar o comando kubectl delete pod sistema-noticias, vamos fazer o que de início: vamos deletar o Pod atual do nosso sistema de notícias e vamos recriar ele utilizando um Deployment, um comando kubectl apply -f e vamos passar o nosso arquivo.

	kubectl apply -f sistema-noticias-deployment.yaml

[05:52] Eu vou criar o nosso db-noticias-deployment.yaml na lista de arquivos, clicando no botão “New File” no canto superior esquerdo da tela, pra praticarmos, mais uma vez, e vamos colocar a mesma coisa. Vamos copiar o conteúdo do arquivo db-noticias.yaml e dentro dele caber no nosso Deployment.

[06:23] E nas nossas especificações, repare que vou mostrar que não precisamos definir as réplicas, porque por padrão ele já vai definir como 1.

[07:18] Então, 
	
	kubectl apply -f db-noticias-deployment.yaml 
	
foi devidamente criado. Se dermos um kubectl get pods, tudo devidamente funcionando.

[07:31] Só para finalizar, assim como fizemos com o portal, kubectl annotate e vamos definir o nosso Deploy e vamos chamar qual deles queremos definir. Temos três, que é o nosso portal-noticias-deployment, db-noticias-deployment e o nosso sistema-noticias-deployment.

[07:51] Então, vamos definir o nosso sistema-noticias-deployment, porque a nossa CHANGE-CAUSE, assim como fizemos com o nosso portal-noticias, então, kubernetes.io/change-cause e vamos colocar: Subindo o sistema na versão 1, só para tudo ficar padronizado.

[08:17] Pronto, está aqui. Se dermos nosso comando kubectl rollout history deployment portal-noticias-deployment, mas, se trocarmos, invés de portal-noticias para sistema-noticias e está aqui subindo o sistema na versão 1.

[08:33] Vamos finalizar fazendo a mesma coisa para o nosso banco, então, kubectl annotate deployment sistema-noticias-deployment kubernetes.io/change-cause=”Subindo o banco na versão 1” e não queremos alterar o nosso sistema-noticias-deployment e sim nosso db-noticias-deployment.

[08:48] Então, colocamos db-noticias-deployment e ele vai anotar também. Se exibirmos o nosso db-noticias-deployment tudo está funcionando sem nenhum problema. E agora, temos todo esse controle, se caso alguma de nossas aplicações parem de funcionar, tudo vai voltar da maneira que estava.

[09:12] Então, o que pode acontecer? Vamos voltar para o nosso portal e vamos entender o que aconteceu. Vou dar um "F5" e cadê as nossas notícias que tínhamos registrado aqui? Se eu vier no painel e fizer o login, cadê as notícias que eu cadastrei? É essa pergunta que temos agora para responder.

[09:36] Então, sumiu tudo que fizemos porque os Pods por serem efêmeros eles não tem nenhum dado armazenado neles, porque eles estão prontos para serem armazenados, criados e destruídos.

[09:49] Então, já temos a primeira pergunta: aonde foi parar as nossas notícias? Como podemos persistir os dados em caso de falhas? Precisamos ter alguma maneira de, caso um container dentro de um Pod reinicie ou o Pod todo reinicie, precisamos ter o acesso às informações que já estavam lá.

[10:11] E, para isso, vamos começar a ver um conceito novo no próximo vídeo que vão ser os Volumes Persistentes, Storage Classes. Um conteúdo bem vasto sobre toda essa questão de armazenamento no Kubernetes. E para isso eu vejo vocês no próximo vídeo. Até mais!
** Aula 02 - Persistindo dados com o Kubernets
*** O que aprendemos nessa aula
Como criar Volumes através de arquivos de definição
Volumes persistem dados de containers dentro de pods e permitem o compartilhamento de arquivo dentro dos pods
Que Volumes possuem ciclo de vida independente dos containers, porém, vinculados aos pods
Como criar PersistentVolumes através de arquivos de definição
PersistentVolumes persistem dados de pods como um todo
PersistentVolumes possuem ciclo de vida independente de quaisquer outros recursos, inclusive pods
Como criar e para que servem os PersistentVolumeClaims
Que precisamos de um PersistentVolumeClaim para acessar um PersistentVolume
*** Persistindo dados com volumes (no Windows)
"./Kubernetes: Deployments, Volumes e Escalabilidade/Imagens/Aula02_video01_img01.png" à "Aula02_video01_img08.png"
Ver arquivo: "./Arquivos-k8s/kubernetes-parte2-Aula2/pod-volume (Windows).yaml"

[00:00] Nós estamos enfrentando um problema de persistência de dados, porque, o que acabou de acontecer? Nós pegamos alguns arquivos das nossas notícias, colocamos dentro do nosso Pod, dentro do container desse Pod e quando este Pod é encerrado, todos os arquivos dele são perdidos.

[00:18] Isso porque eles, ao serem executados nativamente, não tem nenhuma maneira, até então, de nós utilizarmos e persistir os dados de algum modo.

[00:28] Porque nós sabemos que eles são efêmeros, eles devem estar prontos para serem trocados a qualquer momento e isso implica numa situação um pouco mais delicada.

[00:39] Então, nós precisamos ver o que? Nós precisamos começar a estudar as questões da persistência de dados, primeiro no âmbito dos containers dentro de um Pod para que nós consigamos partir para conhecimentos mais aprofundados, que seria a persistência de dados no próprio Pod.

[00:57] Então, o que acontece aqui? Nós já vimos lá do Docker que nós conseguimos compartilhar dados, arquivos de maneira geral entre containers.

[01:08] O que acaba acontecendo? Nós precisamos de início, como eu acabei de falar para vocês, ter alguma maneira também do Kubernetes de compartilhar dados entre containers.

[01:19] Mas, entre containers de um mesmo Pod. Nós precisamos estudar isso para entender a base e depois partir para o avançado.

[01:27] E para isso nós temos os nossos recursos que são os Volumes, os Persistents Volumes, os Persistent Volume Claim e os Storage Classes.

[01:37] Nós vamos começar falando sobre os Volumes. E qual é a peculiaridade deles? Bom, voltando ao nosso problema inicial, mais básico, nós temos um container ou alguns containers dentro de um Pod e queremos compartilhar um arquivo entre eles.

[01:55] Se nós queremos compartilhar arquivos entre containers no Docker, nós fazemos o que? Nós criamos Volumes. No Kubernetes nós fazemos a mesma coisa, só que a peculiaridade dele é que o ciclo de vida é independente dos containers do Pod, mas é dependente do Pod.

[02:14] Então, isso quer dizer que se nós vincularmos, colocar algum arquivo dentro desse Volume, dentro dos containers compartilhando esse Volume e algum desses containers dentro do Pod falhe, mas, o Pod ainda esteja em execução, esse Pod ainda existe.

[02:33] E caso nós consigamos fazer o reinício manual desse container ou mantenha ele ainda em estado parado, mas tenha outros containers em execução, os arquivos ainda estão persistidos, por mais que este container tenha falhado.

[02:46] Porque frisando, o ciclo de vida desse Volume é independente do container e sim dependente do Pod. Isso quer dizer que se esse segundo container aqui falhar, ou seja, todos os containers desse Pod estão falhando, o Pod vai falhar, logo, o volume vai ser removido, porque o ciclo de vida dele é dependente do Pod.

[03:07] Mas, o que vai acontecer com o arquivo? Bom, depende, se nós olharmos aqui na documentação oficial do Kubernetes, o que acontece? Nós conseguimos ver toda a questão dos Volumes e que os containers são efêmeros assim como os Pods e tudo mais.

[03:24] E ele conta uma breve história sobre estes tipos de recursos, bem básico que é o básico que eu contei para vocês.

[03:31] E mais aqui embaixo ele tem os tipos de Volumes, então, ele fala que o Kubernetes suporte diversos tipos, por exemplo, o awsElasticBlockStore, azureDisc, azureFile, vários.

*[03:44] O que nós vamos usar para exemplificar a nossa utilização de Volumes é o hostPath, que é que nem o que vocês fizeram lá no curso de Docker.*

*[03:53] Então, nós fazemos o bind de um diretório do nosso host para um diretório de dentro do nosso container do nosso Pod, nesse caso do Kubernetes.*

[04:06] E como que ele funciona? Se nós clicarmos no link do “hostPath” e ver, é exatamente dessa mesma maneira, o hostPath monta um caminho ou arquivo do nosso host e monta ele dentro do nosso Pod, dentro dos containers desse Pod.

[04:20] E enquanto o Pod ficar vivo, esse Volume vai existir, e como é que nós fazemos a criação desse hostPath? É bem simples, como que nós criamos um volume básico no Kubernetes?

[04:32] Então, nós precisamos criar um arquivo de definição para ele. Vou criar um arquivo chamado pod-volume.yaml e dentro dele nós vamos definir um Pod mesmo para ser mais prática, mais simples a definição, funcionaria da mesma maneira para um ReplicaSet ou para um Deployment.

[04:49] Vamos colocar aqui o kind que é um Pod e no metadata vamos definir um name para ele que vai ser o nosso pod-volume.

[04:56] Nas especificações vamos colocar aqui as informações do container que nesse caso nós vamos fazer o que? Nós vamos ter o mesmo cenário da nossa apresentação. Vou encerrar a tela e colocar a animação da apresentação novamente para nós termos a ideia inicial.

[05:10] Nós almejamos criar dois containers dentro de um Pod e um Volume para fazer o compartilhamento entre esses dois containers, é esse o nosso objetivo para entender a utilização de Volumes.

[05:24] Então, vamos lá, como nós declaramos dois containers dentro de um Pod? É bem simples, basta seguir o que nós já viemos fazendo, nós definimos um nome para ele, vamos chamar ele de nginx-container, uma imagem, vou utilizar a versão nginx-latest.

[05:41] E agora, repara que o nome containers já está até no plural, basta nós fazermos outra definição de nome e imagem. Esse traço no início da especificação name indica uma nova posição, como se fosse um array, uma nova posição nesse array para nós adicionarmos um novo campo.

[05:59] Então, nós estamos adicionando aqui um novo campo, um novo container, por exemplo, vamos utilizar uma outra especificação bem famosa, por exemplo, o Jenkins. Vamos colocar o jenkins-container e utilizar a versão não latest, vamos utilizar uma versão mais leve, por exemplo, a alpine, irrelevante isso, mas só para nós definirmos aqui.

[06:20] E agora, falta nós definirmos o nosso Volume, mas, como eu falei para vocês os Volumes tem ciclo de vida atrelado ao Pod, a especificação do Pod e não aos containers, então, faz todo sentido nós alinharmos com os containers a definição dos Volumes desse Pod.

[06:40] Que é bem simples, basta nós definirmos um nome para ele, como por exemplo, primeiro-volume e definir qual é o tipo, o que nós queremos fazer? Nós queremos criar um hostPath e agora nós precisamos definir qual é o caminho dele no nosso host.

AP: O professor do curso fez essa configuração no Windows - a configuração no Linux será apresentada em aula mais à frente.
[06:56] O caminho vai ser para /c/Users/Daniel/Desktop/primeiro-volume. Eu criei essa pasta na minha área de trabalho e ela vai ser o nosso Volume para nós.

[07:12] E agora, a questão é bem simples, basta nós também falarmos que o tipo desse Volume é um diretório, sem nenhum mistério.

[07:20] Só que falta algumas coisas ainda, como por exemplo, nós precisamos informar agora que cada um desses dois containers que nós definimos dentro do nosso Pod, vai montar esse Volume chamado primeiro-volume em algum lugar deles.

[07:36] Então, dentro da definição de cada um desses containers, nós precisamos colocar aqui esse volumeMounts e dentro dele basta definir qual é o caminho dentro do nosso container. Pode ser, por exemplo, volume-dentro-do-container, sem nenhum mistério.

[07:53] E qual é o Volume que nós queremos montar? É o primeiro volume, então, o nome do Volume que nós queremos montar é primeiro-volume, sem nenhum mistério.

[08:06] E como nós queremos fazer isso para o nosso segundo container basta nós virmos no volumeMounts colar e tudo feito, sem nenhum mistério.

[08:15] Por fim, basta vocês fazerem o seguinte, o pessoal que está no Windows comigo vai ter que fazer essa declaração como eu fiz para o caminho, não vai ser c:// vai ser no formato do Unix /C/Users/ o seu usuário/pasta que você quiser compartilhar.

[08:33] Eu, no caso, coloquei essa aqui, /C/User/Daniel/Desktop/primeiro-volume. Eu vou mostrar para o pessoal do Linux, vai ter uma pequena diferença que já eu mostro, mas aqui no Windows ainda vocês vão precisar fazer o seguinte.

[08:43] *Aqui vocês, dentro do Docker, das configurações do Docker que você podem abrir com settings, basta vocês fazerem o que? Vocês precisam desativar, caso esteja ativa essa opção de utilizar o WSL 2, vai precisar reiniciar o Docker, sem nenhum problema.*

[09:01] *E depois em resources, basta vocês virem em “File Sharing” e marcar aqui no botão mais “+”, escolher, por exemplo, que nem eu fiz: a pasta C/Users/Daniel/Desktop/primeiro-volume, selecionar pasta e agora essa pasta vai estar compartilhável com o Docker. Sem nenhum mistério, tudo feito.*

[09:23] Com isso feito, basta nós virmos no PowerShell e aplicar esse arquivo, então 

	kubectl apply -f pod-volume.yaml.

[09:34] E agora, ele foi criado sem nenhum mistério, se nós dermos um kubectl get pod agora, repara que nós temos aqui o nosso Pod com 2/2 em execução, sem nenhum problema.

[09:48] Nós temos dois containers em execução e dois estão prontos já e basta nós virmos aqui e fazer o seguinte teste, eu quero executar um comando em modo interativo no meu Pod Volume, mas agora como eu tenho dois containers dentro desse Pod eu preciso explicitar se eu quero fazer isso no meu primeiro ou no meu segundo.

[10:10] E como é que eu faço isso? Só passando o nome. Então --container nginx-container, eu quero fazer o que? Eu quero executar o --bash, sem nenhum mistério.

	kubectl exec -it pod-volume --container nginx-container -- bash

[10:24] Se eu usar um ls, olha que legal, eu tenho o meu volume-dentro-do-container assim como eu defini no mouthPath dentro de VolumeMounts. E ele está mapeado para a minha área de trabalho nessa pasta primeiro-volume.

[10:37] Se eu acessar essa pasta, então cd/volume-dentro-do-container, eu posso, por exemplo, criar um arquivo, um arquivo.txt. Isso significa que agora se eu vier na minha área de trabalho eu tenho o primeiro-volume e aqui está o arquivo que eu acabei de criar as 17:31.

[10:55] Isso também significa que se eu vier aqui agora, sair desse meu container e acessar o meu jenkins-container com a mesma maneira, basta eu colocar o nome dele. O que vai acontecer? Eu também vou ter aqui o meu volume-dentro-do-container e posso, se eu vier aqui comandar um cd volume-dentro-do-container e acessar esse meu Volume se eu der um ls, aqui está o meu arquivo.txt.

[11:18] Inclusive, eu posso criar outro arquivo, por exemplo, outro arquivo.txt e se eu vier mais uma vez na minha área de trabalho, eu tenho agora esses dois arquivos criados. Isso significa que esse Volume vai existir, se nós sairmos daqui, enquanto este meu Pod existir.

[11:38] Como assim? Vamos executar o comando

	kubectl describe pod pod-volume

se nós dermos uma olhada aqui em cima, olha só a definição dele aqui do Volume, esse cara ele só vai existir essa definição só vai existir enquanto este Pod existir. Ele está vinculado diretamente ao nosso Pod.

[12:02] *Mas, como os arquivos já estão persistidos no nosso host, no caso aqui do hostPath, por mais que o Volume deixe de existir em algum momento, os arquivos vão continuar nessa pasta, o que vai ser encerrado é o vínculo.*

[12:16] Então, se eu remover agora este meu pod-volume, volume atrelado a este Pod, se nós voltarmos na nossa apresentação, como os dois Pods que estavam em execução foram removidos, foram parados, tudo, os dois containers, perdão, vinculados a esse Pod foram parados, o Pod também parou.

[12:37] *Logo, o Volume também foi removido, mas nesse caso, o arquivo, como nós fizemos esse mapeamento para a nossa área de trabalho aqui para um lugar persistente no nosso disco, os arquivos vão continuar aqui.*

[12:51] *O que não continua é o Volume em si, se nós fizermos essa mesma declaração de novo, ele vai criar outro Volume para nós, mas que vai seguir a mesma ideia. E como os arquivos já vão existir, vai estar tudo ali sem nenhum problema.*

[13:08] Então, nós precisamos ver agora como que isso vai funcionar no Linux, então, eu vou fazer um vídeo um pouco mais breve em relação ao que nós fizemos no Windows agora, para o pessoal do Linux entender como vai funcionar lá.

[13:20] Então, por esse vídeo aqui é só, eu vejo vocês no próximo vídeo onde nós vamos falar sobre como isso que nós acabamos de fazer vai funcionar no Linux e eu vejo vocês lá. Até mais!
*** Volumes no Linux
Ver arquivo: "./Arquivos-k8s/kubernetes-parte2-Aula2/pod-volume (Linux).yaml"
(e para fazer a criação do diretório de modo dinâmico, ver: "./Arquivos-k8s/kubernetes-parte2-Aula2/pod-volume (Linux) (DirectoryOrCreate).yaml"

Atenção! A imagem do Jenkins no Docker Hub foi alterada! Agora para executá-la, você deve utilizar a imagem jenkins/jenkins:alpine e não a jenkins:alpine!

[00:00] Eu estou no Linux agora para nós entendermos qual vai ser a diferença do Windows para o Linux nesse cenário do nosso terminal.

[00:08] Nós temos já, se eu executar o kubectl get pods tudo da mesma maneira, só falta nós colocarmos o nosso primeiro Volume lá com o pod-volume.yaml para ser executado e nós fazermos o nosso teste.

[00:22] Então, eu vou abrir o arquivo com o pod-volume.yaml e nós temos a mesma definição que eu fiz no Windows, mas, qual vai ser a diferença? É que no Windows nós temos o diretório c:// que nós definimos já no formato do Unix.

[00:40] O que nós precisamos fazer aqui agora? Nós precisamos, a princípio, trocar para algum diretório que nós tenhamos no Linux, por exemplo, para /home/primeiro-volume e assim como nós fizemos no Windows, eu preciso aqui no Linux criar essa pasta.

[00:57] Então, eu vou clicar no botão “New Folder” dentro da minha home e vou criar o meu primeiro-volume. E a minha pasta foi criada, já estou dentro dela.

[01:08] A princípio, se eu vier no terminal agora e executar o comando kubectl apply -f pod-volume o que vai acontecer? Ele falou que foi criado, mas, se nós viermos e dermos um comando kubectl get pods ele está em criação.

[01:24] O ponto é, vamos dar uma olhada para ver uma coisa mais específica na descrição da criação desse Pod, kubectl describe pod pod-volume. O que está acontecendo aqui? Ele deu um erro aqui embaixo, ele deu um warning e um problema de montagem.

[01:42] O que aconteceu? Ele tentou fazer toda essa configuração para o nosso primeiro volume, mas ele falou que não encontrou com o hostPath, com /home/primeiro-volume, ele falou que esse arquivo não é um diretório.

[01:57] Mas, por que se ele está aqui, eu estou acessando ele agora? Porque, como eu falei para vocês, nesse cenário aqui do Linux nós estamos utilizando uma máquina virtual para criar e utilizar o nosso cluster que é o Minikube.

[02:12] Então, a grande questão é que nós precisamos criar dentro dessa máquina virtual, dentro do sistema operacional dessa máquina essa pasta chamada primeiro-volume dentro de home.

[02:24] E como é que nós acessamos essa máquina virtual? Com o comando 

	minikube ssh

e agora nós estamos dentro do Minikube.

[02:32] Então, podemos acessar agora cd /home. Se nós dermos um ls, nós não temos a pasta primeiro-volume aqui, só a pasta docker, então, sudo mkdir e vamos criar a pasta primeiro-volume, vamos dar um "Enter".

[02:50] E agora, se nós sairmos com o "Control + D" de dentro do Minikube, vamos fazer o seguinte, vamos dar um kubectl delete -f no nosso pod-volume, e vamos recriar utilizando o mesmo arquivo.

[03:05] Porque agora nós criamos a pasta e vamos ver o que vai acontecer.

[03:10] Que ele vai deletar, sem nenhum mistério, e agora nós recriamos com o pod-volume.yaml.

[03:18] Vamos dar um kubectl describe pod em cima desse pod-volume e a princípio ele inicializou tudo sem nenhum problema.

[03:30] E nós conseguimos, inclusive, agora dar o nosso kubectl get pods e está aqui em execução os dois. Os dois containers dentro desse Pod, no caso.

[03:40] Mas será que eu preciso sempre acessar e criar manualmente, seja lá qual for o meu host, para criar o meu diretório?

[03:47] Na verdade não, o que eu poderia ter feito aqui é o seguinte, vamos voltar, vou deletar mais uma vez o nosso pod-volume para nós entendermos o que vai acontecer agora.

[03:58] Então, ele foi deletado e agora nós vamos fazer isso aqui, vamos acessar mais uma vez esse arquivo do nosso pod-volume e vamos fazer o seguinte experimento, eu vou colocar no path ao invés de primeiro-volume, eu vou chamar de segundo-volume. (AP: ver arquivo "./Arquivos-k8s/kubernetes-parte2-Aula2/pod-volume (Linux) (DirectoryOrCreate).yaml".)

[04:25] E agora o que vai acontecer? Essa pasta não existe, mas, eu posso falar para o Kubernetes fazer o seguinte, se esse diretório não existir eu posso utilizar a criação, então DirectoryOrCreate, caso esse diretório não exista, ele vai ser criado automaticamente.

apiVersion: apps/v1
kind: Pod
metadata:
    name: pod-volume
spec:
    containers:
      - name: nginx-container
        image: nginx:latest
        volumeMounts:
            - mountPath: /volume-dentro-do-container
              name: segundo-volume
         - name: jenkins
           image: jenkins/jenkins:alpine
        volumeMounts:
            - mountPath: /volume-dentro-do-container
              name: segundo-volume
  volumes:
   - name: segundo-volume
     hostPath:
       path: /home/segundo-volume
             type: DirectoryOrCreate

[04:43] Então, kubectl apply -f e vamos executar aqui o nosso "pod-volume (Linux) (DirectoryOrCreate).yaml" e ele vai criar.

[04:52] Se nós dermos um kubectl get pods aqui, ele vai estar em criação, se nós dermos um kubectl describe pods no nosso pod-volume e que vai acontecer? Ele já inicializou, então se nós viermos aqui agora em execução, sem nenhum problema.

[05:08] E vamos conferir, vou acessar o nosso Minikube e vamos dar um cd/home e um ls, está aqui o nosso segundo volume criado de maneira dinâmica.

[05:19] Então, vai funcionar da mesma maneira, os nossos testes, nós vamos conseguir fazer utilizando o kubectl exec -it no nosso pod-volume e também vamos falar aqui que nós queremos utilizar o nosso nginx-container executando o bash.

[05:40] Dentro dele podemos acessar o nosso diretório volume dentro do container e criar algum arquivo aqui também qualquer.

[05:49] Se nós voltarmos aqui agora ao nosso host e acessar o Minikube com ssh, dentro dele, cd/home e segundo-volume está aqui o nosso arquivo também criado.

[06:02] Então, mesma maneira que vai funcionar, nós só precisamos nos atentar a essa questão da virtualização e também DirectoryOrCreate que nós podemos utilizar para fazer a criação do diretório de modo dinâmico.

[06:14] Então, essa primeira parte que nós vimos de criar volumes para persistir dados entre containers, é isso que nós temos, o Volume, o recurso que o Kubernetes nos dispõe.

[06:26] Mas, nós precisamos também ter maneiras mais específicas de compartilhar e conseguir persistir dados, através desses outros recursos que são um pouco mais complexos. Os Persistent Volumes, os Persistens Volumes Claims e os Storage Classes.
*** Pergunta do curso da Alura
João está trabalhando em um novo projeto que necessita de uma aplicação web. Ele decidiu utilizar Kubernetes para orquestrar e gerenciar os containers que executarão a aplicação. Para que a aplicação possa ler e gravar dados persistentes, ele precisa utilizar Volumes.

Com isso em mente, ele criou um arquivo de definição abaixo

apiVersion: v1
kind: Pod
metadata:
  name: um-pod-qualquer
spec:
  containers:
    - name: nginx-container
      image: nginx:latest
      volumeMounts:
        - mountPath: /pasta-de-arquivos
          name: volume-pod
  volumes:
    - name: volume-pod
      hostPath:
        path: /C/Users/Daniel/Desktop/uma-pasta-no-host
        type: Directory

Qual será o resultado produzido pelo arquivo?
**** Independente da pasta /C/Users/Daniel/Desktop/uma-pasta-no-host existir no host, um volume chamado volume-pod será criado e montado na pasta pasta-de-arquivos dentro do container do Pod.
Alternativa errada! Caso o caminho /C/Users/Daniel/Desktop/uma-pasta-no-host não exista, não conseguiremos criar o nosso Pod. Uma possibilidade seria utilizar o type: DirectoryOrCreate.
**** O arquivo não funcionará.
Alternativa errada!
**** Caso a pasta /C/Users/Daniel/Desktop/uma-pasta-no-host exista no host, um volume chamado volume-pod será criado e montado na pasta pasta-de-arquivos dentro do container do Pod.
Alternativa correta!
**** Um volume chamado volume-pod será criado e montado na pasta /C/Users/Daniel/Desktop/uma-pasta-no-host dentro do container do Pod.
Alternativa errada!
*** assistir "Aula 03 - Storage Classes e StatefulSets" (principalmente a aula "Utilizando o StatefulSet") uma vez que nessa Aula 02 foi usado Google Cloud Plataform, e na Aula 03 é feita a implementação na máquina Local
** Aula 03 - Storage Classes e StatefulSets
*** Pergunta da Alura
Maria está enfrentando problemas em sua infraestrutura de armazenamento. Ela precisa provisionar um grande número de PersistentVolumes de diferentes tipos dentro de um prazo curto, mas está com dificuldades para gerenciar e monitorar tudo isso, até perceber que a utilização de Storage Classes pode ajudar

Qual das afirmativas abaixo mostra uma vantagem da utilização de Storage Classes?
**** Storage Classes executam de maneira mais rápida no cluster.
Alternativa errada! Isso não é uma afirmação válida.
**** Storage Classes fornecem dinamismo para criação de PersistentVolumes conforme demanda.
Alternativa correta!

Alternativa correta
Storage Classes são iguais para todos e quaisquer serviços de cloud providers
**** Storage Classes são iguais para todos e quaisquer serviços de cloud providers
Alternativa errada! Cada cloud provider pode ter sua definição específica.

** Aula 04 - Checando status com Probes
*** O que aprendemos nessa aula
O Kubernetes nem sempre tem como saber se a aplicação está saudável
Podemos criar critérios para definir se a aplicação está saudável através de probes
Como criar LivenessProbes com o campo livenessProbe
LivenessProbes podem fazer a verificação em diferentes intervalos de tempo via HTTP
Como criar ReadinessProbes com o campo readinessProbe
ReadinessProbes podem fazer a verificação em diferentes intervalos de tempo via HTTP
LivenessProbes são para saber se a aplicação está saudável e/ou se deve ser reiniciada, enquanto ReadinessProbes são para saber se a aplicação já está pronta para receber requisições depois de iniciar
Além do HTTP, também podemos fazer verificações via TCP
*** Conhecendo probes
[00:00] Agora vamos falar de um tópico bem importante, mas, que ao mesmo tempo, vai ser bem fácil de entendermos e colocar a mão na massa. E, a questão agora é a seguinte: nós temos uma requisição que pode chegar para esse serviço e ele, nós sabemos, vai fazer o balanceamento de carga entre os três Pods que temos.

(Ver imagem: "./Imagens/Aula04_video01_img01.png")

[00:20] Vai chegar a requisição pro container dentro desse Pod, ele vai fazer algum processamento e vai exibir o resultado, seja uma página web, seja o retorno de alguma API, o que seja, tanto faz.

[00:31] E ele retorna o Status Code, por exemplo, de 200, que a requisição foi ok ou algum de 300 que seja um redirecionamento, qualquer coisa do tipo. Mas, o que importa é que o SVC pode balancear a carga entre qualquer um destes Pods.

[00:49] O que podemos acabar vendo acontecer? Em algum momento este Status Code recebido tem a possibilidade de ser um erro 500 ou algum erro de 404, de Not Found um erro interno no servidor de 500 e isso significa que a aplicação dentro deste Pod não está respondendo de maneira esperada. Ela não está funcionando da maneira que esperávamos.

[01:19] Mas, a pergunta que fica é: o Kubernetes sabe que o Pod está em funcionamento, mas, a aplicação dentro deste Pod não tem como saber, a princípio, se ele deve reiniciá-la ou não. Então, apesar do Pod estar saudável e funcionando, a aplicação dentro do container deste Pod não está respondendo da maneira esperada.

[01:42] Como assim? Vamos visualizar o caso do nosso portal de notícias. Eu vou abrir a abra do console do navegador e se eu recarregar a página vemos, por exemplo, que quando fazemos uma requisição para localhost:30000, recebemos um código de status 200, ou seja, a requisição foi ok.

[02:01] Tivemos tudo sem nenhum problema, mas, pode ser que se tivermos algum código de 400 pra cima e ali na faixa dos 500 também, essa aplicação não esteja respondendo da maneira esperada. Porque, ou nós tomamos um erro de Not Found ou um erro interno ou um Bad Request, qualquer outra coisa do tipo.

[02:21] E para resolver este tipo de problema, nós temos os Configure Liveness, Readiness e Startup Probes. Vamos falar sobre cada um deles, mas, nós vamos começar falando sobre o Liveness Probes, que é nada mais do que uma prova de vida que a aplicação dentro de um container de um Pod está funcionando.

[02:40] Então, o kubelet vai usar um Liveness Probe como um critério para saber quando reiniciar o container de um Pod. Então, quando cair em algum deadlock ou a aplicação está rodando, mas, incapaz de manter progresso. Os cenários que exibimos aqui, por exemplo.

[02:55] E, nesse cenário , o que podemos fazer, o que podemos visualizar? O kubelet, que é aquele componente que mostrei pra vocês lá dos nossos nodes, eles são responsáveis por, através do nosso Liveness Probe, saber se a aplicação deve ser reiniciada ou não.

[03:15] Então, como que podemos informar ao kubelet utilizando os Liveness Probe, que essa aplicação deve ser retornada caso ela não tenha nenhum status favorável http.

[03:26] Ele tem um pequeno guia para nós, mas, reparem que não precisamos declarar um arquivo só para o Liveness Probe. Dentro dele, da definição do nosso container, podemos definir qual é a prova de vida, como garantimos que este container dentro do nosso Pod está vivo.

[03:44] Então, são essas perguntas que precisamos responder. No próximo vídeo vamos colocar a mão na massa e criar o nosso primeiro Liveness Probe, pra sabermos quando o nosso Pod já está pronto com o container dentro dele para ser reiniciado em caso de falhas.
*** Configuração adicionada em portal-noticias-deployment.yaml para configurar os gatinhos de reinicialização dos containers de forma automática pelo Liveness Probe
//Obs: ele entende que respostas ok são as de: 200 <= HttpStatus < 400. Ou seja: Ele indicará falha caso o código de retorno seja menor que 200 ou maior/igual a 400.

          livenessProbe:
            httpGet:
              path: /
              port: 80
            periodSeconds: 10 //A cada quantos segundos eu quero fazer o teste? R:10 segundos
            failureThreshold: 3  //qual o número de falhas que escolho para que quando atingido 
					    //reinicie o container?
            initialDelaySeconds: 20  // quanto tempo deixado para iniciar os
                                      // testes? (para deixar o contatiner subir)
*** Configuração adicionada em portal-noticias-deployment.yaml para configurar o tempo de folga esperado para que o container inicialize
Ver imagem: "./Imagens/Aula04_video02_img01.png"

 Esse teste serve para saber se a aplicação já está pronta para receber requisições.

//Obs: ele entende que respostas ok são as de: 200 <= HttpStatus < 400. 

[00:00] Estamos de volta ao nosso cenário original porque veremos outro problema que tem a possibilidade de acontecer. Temos algumas requisições chegando ao nosso serviço e ele vai fazer o balanceamento entre cada um desses Pods.

[00:14] Mas, o que temos a possibilidade de ver acontecer? Em algum momento, algum desses Pods, por exemplo, tem a capacidade total de falhar. Eles são suscetíveis a erro, ou um container dentre deste Pod também. E o que vai acontecer?

[00:32] Nós já vimos a possibilidade de definir um Liveness Probe para o container desse Pod e ele vai voltar à execução, ou também, se o Pod, como um todo, tiver falhado, temos a possibilidade de usar Deployments, StatefulSets, ReplicaSets para garantir que este Pod vai voltar à execução.

[00:52] Mas, o que importa é que enquanto ele volta à execução, o Pod já está pronto, mas, o container que, também, já subiu, ainda não terminou, não está pronto para receber essas requisições. Ele ainda não terminou de executar os scripts que ele tem para iniciar ou qualquer outra coisa do tipo.

*(Ver imagem citada no começo do tópico)[01:12] Então, precisamos ter uma maneira de garantir quando o container deste Pod estará pronto para receber as requisições e consigamos parar de fazer essa divisão entre só o nosso segundo e terceiro Pod, para que o primeiro receba um sinal de ok e passe a receber as requisições, novamente.*

[01:31] E a questão para fazermos isso é bem simples. Basta definirmos os nossos Readiness Probes, que são bem fáceis e é, basicamente, definirmos uma situação bem parecida, só vão mudar as consequências.

[01:46] Então, vou colocar um Readiness Probe com a mesma declaração de um Liveness Probe, onde eu vou fazer uma requisição utilizando o verbo get para o meu caminho barra na porta 80 a cada 10 segundos e o failureThreshold, onde vimos que no nosso Liveness Probe significa que se ele não conseguir executar este teste três vezes, ele vai reiniciar a aplicação.

[02:14] Nesse caso significa que se ele não conseguir executar esse teste três vezes, na quarta vez, ele vai enviar as requisições, mesmo assim, então ele vai passar a ignorar esse Readiness Probe. Então, podemos definir que um número maior, como, por exemplo, cinco, dez

          readinessProbe:
            httpGet:
              path: /
              port: 80
            periodSeconds: 10         
            failureThreshold: 5     // se ele não conseguir executar esse teste 5 vezes, na sexta vez, ele
					 // vai enviar as requisições, mesmo assim
            initialDelaySeconds: 3    // um tempo inicial, podemos colocar, por exemplo, três segundos depois
						// que o Pod iniciar e o container estiver subido também, nós
						// precisamos esperar três segundos antes de começar a fazer estes
						// testes, que ele vai executar a cada 10 segundos, conforme definido
						// em periodSeconds

** Aula 05 - Como escalar com o Horizontal Pod Autoscaler
*** O que aprendemos nessa aula
Reiniciar a aplicação incessantemente através de ReplicaSets/Deployments nem sempre resolverá o problema
HorizontalPodAutoscalers são responsáveis por definir em quais circunstâncias escalaremos nossa aplicação automaticamente
Como definir o uso de recursos de cada container em um Pod
O que é, e como utilizar um servidor de métricas
Como utilizar um HorizontalPodAutoscaler através de arquivos de definição
*** Escalando pods automaticamente
Ver "./Imagens/Aula05_video01_img01.png" até /Aula05_video01_img06.png

[00:04] Nós temos um Pod sendo gerenciado por um service e as requisições vão chegando para esse service e ele vai enviando para o nosso Pod, até aí nada de novo.

[00:13] Mas, o que nós temos a capacidade de imaginar aqui? Vamos colocar um cenário novo, vamos dizer que nós estamos falando do nosso portal de notícias e agora nós estamos recebendo diversas requisições porque nós colocamos uma notícia nova no nosso portal e várias pessoas agora querem ler.

[00:29] Então, mais pessoas vão passar a acessar o nosso portal. Isso quer dizer o que? Que o nosso Pod vai passar a consumir mais recursos porque ele vai precisar lidar com mais requisições e enviar mais respostas. Isso quer dizer que ele vai ter um consumo maior de processamento, memória e afins.

[00:46] Qual é o problema disso? O problema é que se ele ficar consumindo de vários, por vários momentos, muitos momentos esse recurso, esse processamento, essa memória, ele vai passar a demorar responder os nossos usuários.

[01:00] Porque ele vai estar ali meio que lento por faltar recursos para ele conseguir trabalhar de maneira hábil.

[01:07] E pior ainda, nós temos a possibilidade desse Pod parar de funcionar por falta de recurso e por mais que ele esteja dentro de um Deployment ou de um StatefulSet ou de um ReplicaSet e seja reiniciado, ele vai continuar caindo pela falta de recurso. Então, a nossa aplicação fica comprometida.

[01:28] Como que nós podemos resolver isso? Basta nós adicionarmos mais Pods. Com mais Pods agora nós temos mais poderes de processamento para cada um deles e vamos conseguir trabalhar sem nenhum problema.

[01:40] Então, nós conseguimos responder os nossos usuários de maneira hábil e, caso o número de acessos ao nosso portal caia, basta diminuir o número de Pods em execução. Perfeito!

*[01:53] Só que o problema é: será que temos alguma maneira automatizada de fazer isso? Nós temos e quem vai nos ajudar, esse recurso se chama Horizontal Pod Autescaler. Vamos dar uma olhada na documentação para ver o que esse recurso vai fazer por nós.*

[02:13] Se dermos uma lida na documentação do Kubernetes, ele resume para nós que o Horizontal Pod Autoescaler é um recurso capaz de, automaticamente, escalar o número de Pods em um Deployment, em um ReplicaSet, em um StatefulSet, baseado na observação da CPU, então, nós temos um primeiro ponto que vamos precisar nos preocupar.

[02:31] E ele vai fazer isso de maneira automática, que é o que importa para nós. Então, vamos fazer um Horizontal Pod Autoescaler para o nosso Deployment do nosso portal-noticias, baseado num consumo de CPU dele, nós vamos aumentar ou diminuir, para suprir essa demanda, o número de Pods.

[02:50] Só que, antes de começarmos, repara se voltarmos na documentação e lermos mais uma vez, ele faz isso baseado em métricas, como, por exemplo, a utilização de CPU.

[03:02] Só que nós precisamos informar, nós temos essa necessidade de declarar quanto esse container, dentro desse Pod, consome de CPU. Quanto ele pede de CPU para funcionar?

[03:16] Então, nós podemos definir que dentre as informações que ele requer, por exemplo, ele pede por algum recurso, que é a nossa CPU. Então, podemos colocar que este container exige 10 milicores de CPU para funcionar.

(Ver arquivo: "./Arquivos-k8s/kubernetes-parte2-Aula5/portal-noticias-deployment.yaml" a inserção de:
          resources:
            requests:
              cpu: 10m
)

[03:35] E, como ele pede por esse recurso de CPU, nada mais válido do que colocarmos ele dentro de um campo chamado recursos. E alinhamos tudo para ficar certo.

[03:48] Então, nós estamos falando que cada Pod que tenha um container dentro do resources vai pedir 10 milicores de CPU para funcionar.

[04:00] Então, agora que definimos isso, como fazemos o Horizontal Pod Autoescaler para este Pod? Simples! Assim como os nossos outros recursos da API, se voltarmos à documentação, nós vemos que o Horizontal Pod Autoescaler também é um objeto da API.

[04:17] Então, nós podemos criar para ele um arquivo de definição no terminal, que vou chamar de portal-noticias-hpa.yaml. Dentro dele podemos definir a versão da API, que se olharmos com clareza, vemos que tem ou um autoescaling/v1 ou um autoescaling/vbeta2.

(Ver arquivo: "./Arquivos-k8s/kubernetes-parte2-Aula5/portal-noticias-hpa.yaml")

[04:37] Nós vamos utilizar a v2beta2 porque toda a documentação está começando a ser mais baseada nela. Ela já está em vias, em ficar 100% estável e ela tem novas instruções e mais enxutas também, então, e por isso vamos utilizar ela.

ATENÇÃO: Desde a gravação do curso a versão da API foi atualizada e a que é suportada para o Horizontal Pod Autoscaler é a v2. A beta já está desuso. Sugerimos que vc utilize a v2 autoescaling/v2. para continuar com a videoaula.

[04:55] Então, vamos definir nossa versão da API, o tipo do que nós queremos criar é um HorizontalPodAutoescaler e ele tem um metadata, assim como nos outros recursos e vamos dar o nome para ele de portal-noticias-hpa.

[05:10] Nas especificações dele, nós precisamos nos preocupar com o seguinte: nós queremos que este Horizontal Pod Autoescaler faça o que? A referência ao nosso portal de notícias. Então, eu preciso definir à quem eu quero fazer a referência.

[05:31] Então, qual é o meu alvo que eu quero fazer referência? Mais além, quem eu quero escalar automaticamente? Eu preciso informar qual é a versão da API do meu alvo. Então, qual é a versão do API do meu alvo, que é esse Deployment? É apps/v1.

[05:49] Qual é o tipo do que eu quero escalar automaticamente? É um Deployment. Qual é o nome desse Deployment? É portal-noticia-deployment.

[06:01] E agora, eu preciso informar o seguinte: qual o número mínimo e máximo de réplicas que eu quero ter para esses Pods do meu Deployment? Eu quero manter sempre um número mínimo de uma réplica e nunca vou poder passar, por exemplo, de 10.

[06:17] Então, por mais que chegue 1 trilhão de requisições e eu precise colocar 200 Pods, eu não vou passar, nunca, de 10.

[06:28] E eu preciso, agora, informar quais são as métricas que eu quero definir para esse Horizontal Pod Autoescaler. Então, o que eu quero definir como o tipo de métrica?

[06:37] Eu quero, baseado nos recursos, não nos recursos do Kubernetes em si, e sim nos recursos do sistema, porque temos recursos de processamento, memória e afins; eu quero definir qual eu vou utilizar.

[06:50] Então, qual é esse recurso que eu quero usar como critério? Eu quero utilizar um recurso chamado CPU. E o que eu almejo com esse recurso? Eu quero que baseado na utilização dele, ele mantenha sempre um uso médio de 50%, por exemplo.

[07:10] 50% de que? Por isso que nós definimos esses 10 milicores. Então, se o consumo médio da nossa aplicação desse Deployment chegar a mais de 5 milicores, o nosso Horizontal Pod Autoescaler vai fazer a mágica dele acontecer, ele vai criar mais Pods para que o consumo médio não passe de 50% de 10 milicores, então, para que ele não passe de 5 milicores, ele fique nessa faixa.
(AP: Pro da próxima aula: queremos *manter um consumo médio de 50% da CPU, que é requisitada por cada Pod, por cada container dentro do Pod.*)

[07:47] Então, vamos fazer isso agora. Salvamos o nosso portal-noticias-deployment com a mudança dele e o nosso Horizontal Pod Autoescaler. Vamos, então, aplicar essas mudanças no PowerShell kubectl apply –f vou passar o nosso portal-noticias-deployment.yaml e agora vou passar o nosso kubectl apply –f e vou passar o nosso portal-noticias-hpa.yaml.

[08:18] Agora, nós criamos. E ele foi criado. Se nós dermos, agora, um kubeclt get hpa, de Horizontal Pod Autoescaler, o que vai acontecer? Que legal!

[08:27] Ele está falando que nós temos o nosso Horizontal Pod Autoescaler, que faz referência a este nosso Deployment do portal-notícias, mas, se temos um olhar de que, por enquanto, ele ainda não reconheceu as nossas réplicas, isso é um primeiro problema, mas, ele também não sabe quanto estamos utilizando daqueles 50% que nós definimos.

[08:50] Vamos dar um comando, de novo, para ver se alguma coisa muda. Temos um positivo, ele já identificou as três réplicas, que por padrão, nós definimos no nosso arquivo. As três réplicas estão em execução, perfeito, mas, ele ainda não consegue identificar quantos por cento do processamento está sendo usado.

[09:11] Vamos tentar mais uma vez e nada. Vamos fazer o seguinte, vamos dar uma olhada, vamos ser abelhudos e vamos descrever o nosso Horizontal Pod Autoescaler chamado portal-noticias-hpa.

[09:25] Olha o que aconteceu. Vamos dar uma olhada no que está descrito após o comando. Ele foi incapaz de pegar as métricas para recurso de CPU. Ele não foi capaz de pegar esses dados da API de métricas. O servidor não pôde encontrar esse recurso pedido. E aqui ele fala que essa métrica foi inválida e nós precisamos definir o que é uma CPU.

[09:50] Então, temos um pequeno problema. O que é esse servidor de métricas que ele falou para nós. Como conseguimos utilizá-lo para fazer o nosso Horizontal Pod Autoescaler funcionar? Isso vamos ver no próximo vídeo.

AP: Adianto a pergunta do curso: Por qual motivo o HorizontalPodAutoscaler pode não conseguir identificar o consumo de recursos dos pods gerenciados?
R: Não foi definido um servidor de métricas para que haja o funcionamento da maneira esperada.

*** Utilizando HPA no Windows
(AP: Arquivos citados aqui em: "./Arquivos-k8s/kubernetes-parte2-Aula5-Extra/". Para rodar o script de bash no Windows pode-se usar o Git bash (https://git-scm.com/download/win).)
 
[00:00] Estou aqui no Github na página do metrics-server, que é um repositório do kubernetes-sigs e nós temos as informações necessárias para utilizarmos um servidor de métricas no nosso cluster.

[00:15] Se viermos na documentação, em Kubernetes Metrics Server, temos uma questão bem simples de caso de uso. Nós podemos usar um Servidor de Métricas para definir, basear as nossas informações de consumo de CPU e memória para utilizar o Horizontal Pod Autoescaler e, também, fazer esse ajuste de maneira automática, conforme recursos necessitados pelos containers.

[00:37] Então, é basicamente o que queremos. E como utilizamos ele? É bem simples, é só descermos até a descrição de Deployments, clicar na parte Metrics Server releases e nós vamos baixar a versão v0.37, basta clicar mais embaixo em components.yaml e ele vai fazer o download para nós.

[00:56] Eu vou já arrastar ele direto para dentro do Visual Studio Code e você que está utilizando o Windows comigo, (teremos um vídeo somente para o Linux), vamos precisar fazer o seguinte.

[01:08] Se você reparar o trecho descritivo da apiVersion é nada mais do que um arquivo de definição, que nós já viemos trabalhando, mas, mais abaixo temos a parte toda da definição do nosso Metric Server, que está, exatamente, descrita no API.

//código omitido

---
apiVersion: v1
kind: ServiceAccount
metadata:
    name: metrics-server
    namespace: kube-system
---

//código omitido


[01:22] Dentro dele, nós temos essa parte em que ele define os argumentos que serão passados para este Pod do nosso Metric Server.

[01:30] E, no caso do Windows, nós vamos precisar fazer o seguinte: mais aqui embaixo na documentação, ele fala na questão de configuração que, caso nós não tenhamos o objetivo de utilizar, fazer uma verificação de certificados, nós podemos desabilitar com essa flag --kubelet-insecure-tls.

[01:50] No caso do Windows, nós vamos precisar definir essa flag dentro do nosso arquivo de definição. Então, um traço aqui em args e definimos os --kubelet-insecure-tls. Basta, agora, darmos um kubectl apply -f no PowerShell e passar o nosso arquivo recém baixado e editado, que é o nosso componets.yaml.

[02:13] Dando um "Enter" ele vai criar todos os recursos definidos lá dentro e agora, o que vai acontecer? Temos o nosso kubectl get hpa e ele continua um tanto quanto engraçado, isso não é um erro, ele continua falando que não está identificado.

[02:32] Então, mais uma vez, vamos dar um kubectl describe hpa e passar o nosso portal-noticias-hpa. Ele, há 51 segundos tentou, de novo, pegar, mas, ainda não conseguiu, então, o que iremos fazer, o que precisamos fazer? Precisamos esperar ele terminar de carregar todas as informações, para que ele consiga ler todos os detalhes de consumo de CPU.

[02:59] Isso é “um bug", que ele demora a fazer esse reconhecimento, então, assim que ele terminar, voltamos e seguimos com nosso Horizontal Pod Autoescaler.

[03:10] Então, o que temos aqui? Temos que nós estamos utilizando 10% e não 50%, então, como estamos utilizando abaixo do nosso alvo, do nosso limite, temos, apenas, um Pod, uma réplica.

[03:25] Então, como será que ele vai se comportar se nós colocarmos mais recursos sendo consumidos, se nós enviarmos diversas requisições para nossa aplicação do nosso local host, que neste caso, definimos na porta 30000, o nosso portal.

[03:40] Então, eu vou no nosso Visual Studio Code fazer o seguinte: eu tenho um arquivo, que é um scriptshell, que vai enviar diversas requisições para o nosso local host na porta 30000, que é onde foi definido nosso portal de notícias, e vou fazer isso enviando indefinidamente. Só que para executarmos este script com SH, nós precisamos ter alguma ferramenta no Windows, que seja capaz de executá-la.

[04:07] Nesse caso, eu vou utilizar o Git Bash. Então, vou deixar o link de download para vocês fazerem sem nenhum problema da ferramenta e para vocês poderem executar. Eu vou acessar o nosso diretório, que é o nosso kubernetes-alura e para executar ele é bem simples.

[04:24] Eu vou colocar SH, vou colocar o nome do nosso script, que é stress.sh, e eu preciso definir, ele recebe um parâmetro aqui, o nosso intervalo. Eu vou colocar de 0.001 segundos e, por fim, vou colocar para vermos a saída dele, para imprimirmos tudo certo neste arquivo de saída.

[04:45] Dou um "Enter" e ele vai começar a executar. E para sermos mais gananciosos, eu vou abrir o outro Git Bash e vou fazer a mesma coisa, executar dois ao mesmo tempo. Vou acessar o nosso kubernetes-alura e vou executar o SH também, no nosso stress.sh, a cada 0.0001 segundos e salvar em outro arquivo de saída.

[05:09] Agora, nós vamos observar no nosso PowerShell o que? Eu vou colocar para analisarmos o nosso Horizontal Pod Autoescaler com o --watch:

	kubectl get hpa --watch

 e se nós repararmos ele já está com consumo de 30%, em relação ao alvo, e conforme esse consumo for subindo, nós vamos ver a mágica acontecer.

[05:26] Então, em algum momento, esse consumo vai ficar tão visível para o nosso Pod, para o nosso container, que nós vamos precisar criar mais Pods. Então, como mais uma vez, não temos a garantia de quanto tempo ele vai demorar em nos exibir isso, eu vou fazer um pequeno corte no vídeo e quando estiver vendo os resultados, eu volto para comentarmos.

AP: Ver imagem: "./Imagens/Aula05_video02_img01.png"

[05:48] O que está acontecendo é que tivemos esse consumo todo. Eu já vou parar para irmos dando tempo dele voltar ao normal; nós tivemos um consumo que ele foi subindo, ficou 30%, e 30% ainda está abaixo do nosso alvo, ele continuou com um Pod, com uma réplica.

[06:11] Ficou em 100% ele viu que é melhor criar agora mais uma réplica e foi o que ele fez, então, ele se manteve e conforme a demanda foi subindo, ele viu que ainda estava longe do alvo, que era de 50%, e ele criou mais uma réplica.

[06:27] E agora, como eu interrompi toda essa criação e envio de requisições, a ideia é que, aos poucos, ele vai voltar ao estado inicial de uma réplica, porque, todo o nosso recurso, que era cobrado, não precisa mais, porque não estamos mais enviando requisição, e ele vai voltar, automaticamente, para uma réplica.

[06:49] Então, reparem que já caiu aqui, ele atualizou o consumo para 53% e ele vai continuar nessa queda, até voltar para o 10%, que era o padrão dele, utilizando uma réplica, apenas.

[07:03] Então, o que conseguimos fazer: definindo, não o nosso arquivo de stress, mas, o nosso Horizontal Pod Autoescaler, nós definimos que queremos *manter um consumo médio de 50% da CPU, que é requisitada por cada Pod, por cada container dentro do Pod.*

[07:21] Nesse caso, de 10 milicores, e sempre que ficarmos acima desse consumo, ele vai colocar mais Pods, tendo um limite de 10, como definimos, para que esse consumo seja ajustado.

[07:37] Então, o máximo que chegamos foi três, mas, ele agora vai regredir, aos poucos, como não está recebendo mais nada, ele vai ficar abaixo desses 50%, que é a nossa média, e vai voltar para o estado inicial dele, que é de uma réplica, a aplicação está ociosa.

[07:51] Então, por essa aula é só, por esse vídeo. Eu vou, agora, mostrar para o pessoal do Linux como vai ser a pequena diferença na questão do IP, que teremos que utilizar no nosso arquivo e, também, como vai funcionar no Minikube. Então, por esse vídeo aqui é só e eu vejo vocês no próximo e até mais!
*** Utilizando HPA no Linux
[00:00] Assim como no Windows, nós também vamos precisar de um servidor de métricas no Linux. E o que ele é, caso você não tenha visto o vídeo anterior ou esteja relembrando agora?

[00:10] Ele nada mais é do que uma aplicação usada para fazer nossa escalabilidade horizontal em conjunto com o nosso Horizontal Pod Autoescaler, baseado no nosso consumo de CPU e memória.

[00:22] Então, ele é o responsável por ser o servidor de métricas, o nome já diz. Ele vai ser responsável por servir essas informações para todos os nossos recursos dentro do nosso cluster.

[00:34] E como fazemos para utilizar esse servidor de métricas? No Windows, nós precisamos utilizar esse arquivo de componentes descrito na documentação, que nós declaramos e definimos e aplicamos no nosso cluster. No caso no Linux, nós estamos utilizando o Minikube e ele, no Linux, tem diversas possibilidades para utilizarmos.

[00:54] Como, por exemplo, se dermos uma olhada na documentação ele já tem uma parte voltada apenas para as extensões, então, se nós executarmos esse comando 

	minikube addons list

reparem o que ele vai nos mostrar. Ele mostra uma série de extensões que podemos utilizar e estão desabilitadas, a maioria.

[01:17] Dentre elas, nós temos o servidor de métricas e para utilizar ele é bem simples, basta habilitá-lo com comando 

	minikube addons enable metrics-server

E, a partir de agora, nós vamos ter isso habilitado, só que, antes de mais nada, antes de habilitarmos, eu vou mostrar para vocês que nós estamos com o mesmo estado que tínhamos no Windows. 
(Ver imagem: "./Imagens/Aula05_video03_img01.png")

[01:40] Nós temos os nossos Pods, das nossas aplicações, e temos também o nosso Horizontal Pod Autoescaler, já em execução com o arquivo que definimos. Então, agora vamos dar o nosso minikube addns enable metrics-server.

[01:58] E, agora, ele vai habilitar esse recurso para nós. Bem simples. O ponto agora é que assim que ele terminar de sincronizar, nós vamos ter o nosso alvo definido dentro do nosso cluster, quanto de consumo estamos tendo para os containers dos Pods desse Deployment.

[02:16] E como não sabemos quanto tempo ele vai demorar para fazer essa sincronização, eu vou pausar o vídeo e quando ele começar a exibir os resultados, eu volto para nós seguirmos.

[02:25] Pessoal, então foi. Se eu executar o comando kubectl get hpa ele está utilizando 0% dos nossos 50% definido, da nossa média, e ele está utilizando uma réplica, que é o mais próximo que ele consegue manter dentro da nossa média de consumo de 50%, não tem como ele consumir de graça para ficar mais perto da nossa média.

[02:46] Então, o máximo que ele tem é 0% para ficar próximo dos nossos 50%, então, ele está usando só uma réplica. Então, o que vamos fazer agora? Vamos estressar a nossa aplicação e para gente utilizar, fazer esse estresse, eu tenho um arquivo que é o nosso stress.sh e nele precisamos definir o IP do nosso nó.

[03:11] No caso da minha máquina, quando eu criei esse cluster com o Minikube, eu vou abrir uma nova aba do terminal, se dermos um kubectl get nodes -o wide foi aqui que ele definiu para nós esse IP no INTERNAL-IP 192.168.99.106. 
(AP: Ver imagem: "./Imagens/Aula05_video03_img02.png")

[03:33] Então, é esse IP que vou colocar aqui na porta 30000, que é onde definimos que o nosso portal de notícias está em execução. Então, dito isso, temos o nosso script e vamos executá-lo.

[03:46] Vou colocar o nosso bash e vou passar o nosso arquivo, o nosso script de estresse, para ele executar a cada 0.001 segundos, e vou salvar e sair do out.pxt.

[04:01] Vamos ver a mágica acontecer. Ele vai começar a enviar diversas requisições e agora, no outro terminal (vou colocar os dois lado a lado) vou executar o kubectl get hpa --watch e nós vamos ver a mágica acontecer.
(AP: Ver imagem "./Imagens/Aula05_video03_img03.png")

[04:19] Eu vou expandir o terminal direito mais um pouco, porque ele é mais relevante e nós vamos ver que ele está utilizando, até então, ainda um Pod, uma réplica dele, porque o consumo ainda está em 0%, mas, aos poucos ele vai começar a atualizar esse valor porque a nossa aplicação está recebendo diversas requisições, então, vamos ver os resultados acontecendo.

[04:41] E o que ele vai fazer? Será que ele vai criar muitos Pods a mais para manter esse consumo médio em 50%? Vamos dar uma pausa no vídeo e assim que ele começar a exibir os resultados voltamos para analisar.

[04:57] O que aconteceu foi que as requisições pararam de ser enviadas (vou maximizar esse segundo terminal à direita, clicando e arrastando com o mouse na extremidade esquerda da tela deste terminal).

[05:02] Ele começou a enviar tantas requisições e o consumo foi ficando tão acima da nossa média esperada de 50% em TARGETS, ficando na faixa de 500%, por exemplo, que ele foi precisando criar mais e mais réplicas, até chegar ao nosso limite de 10, onde ele não pôde mais passar, porque esse era o nosso máximo.

[05:20] E, a partir daí, como no nosso outro terminal eu já parei de enviar as requisições, ele vai passar a diminuir esses recursos.
(AP: Ver imagem "./Imagens/Aula05_video03_img04.png")

[05:30] Porque como eles não estão sendo mais utilizados da mesma maneira de antes, reparem que ele já caiu pra 5%, a ideia é que ele vai começar a diminuir, aos poucos, o número de Pods que estão em execução, para não ficar nenhum Pod ocioso ali, para manter tudo da maneira mais básica possível.

[05:49] Então, mais uma vez, como agora paramos de enviar as requisições e não sabemos o tempo que ele vai demorar para atualizar, eu vou pausar o vídeo, de novo, para assim que ele voltar e tivermos um resultado mais enxuto, de quantos Pods ele está utilizando, eu volto para analisarmos.

[06:09] E voltamos ao nosso estado inicial de um Pod, apenas, porque nosso consumo voltou para 0%. Perfeito, pessoal!

*[06:17] Então, agora o que conseguimos fazer, tanto no Windows quanto no Linux, nós conseguimos definir, automaticamente, critérios para que nosso cluster consiga automatizar se ele vai colocar mais ou menos Pods conforme o consumo de recursos que nós temos.*

[06:35] E isso de maneira bem simples e fácil, através de um arquivo de definição. Então, nós vamos terminar essa aula agora, onde nós vimos como definir um Horizontal Pod Autoescaler e utilizar um Servidor de Métricas tanto no Windows quanto no Linux.
*** VerticalPodAutoscaler
Além do HorizontalPodAutoscaler, o Kubernetes possui um recurso customizado chamado VerticalPodAutoscaler. O VerticalPodAutoscaler remove a necessidade de definir limites e pedidos por recursos do sistema, como cpu e memória. Quando definido, ele define os consumos de maneira automática baseada na utilização em cada um dos nós, além disso, quanto tem disponível ainda de recurso.

Algumas configurações extras são necessárias para utilizar o VerticalPodAutoscaler. Mais informações podem ser obtidas nesse link (https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)
** AP
Tanto o replicaset quanto o deployment rodam várias instâncias do pod na mesma porta. Vale lembrar que cada pod tem uma porta diferente.

[09:28] *Então, no fim das contas, a boa prática, o mais comum que vocês irão ver quando vocês forem criar Pods é criar eles através de Deployments, que eles já vão permitir todo esse controle de versionamento e também os benefícios de um ReplicaSet.*
* Dúvidas gerais
** Pergunta ao ChatGPT 4
*** o que é um node no estudo de k8s?
Um "node" no contexto do Kubernetes (k8s) é uma máquina física ou virtual que hospeda os contêineres (aplicações) que são gerenciados pelo cluster do Kubernetes. Cada node é gerenciado pelo plano de controle do Kubernetes e contém os serviços necessários para executar os pods, que são as menores unidades gerenciáveis, responsáveis por hospedar os contêineres.

Os nodes podem ser classificados em dois tipos principais:

1. **Master Node (ou Control Plane Node):** Este é o node que gerencia o estado do cluster Kubernetes. Ele executa vários componentes críticos para o gerenciamento do cluster, incluindo o kube-apiserver, etcd, kube-scheduler e kube-controller-manager. O Master Node toma decisões sobre o cluster, como agendamento de pods, respostas a eventos do cluster, etc.

2. **Worker Node:** Os Worker Nodes são os nodes onde os contêineres de aplicativos são realmente executados. Eles são gerenciados pelos Master Nodes e contêm os serviços necessários para executar os pods, incluindo o kubelet, que comunica com o Kubernetes Control Plane, e o kube-proxy, um proxy de rede que mantém as regras de rede nos nodes e permite a comunicação entre os pods dentro e fora do cluster.

Cada Worker Node pode executar múltiplos pods, e o Kubernetes automaticamente gerencia o agendamento e a execução dos pods nos nodes com base em vários fatores, como recursos disponíveis e restrições de política. Isso permite que o Kubernetes seja altamente disponível e escalável, facilitando o gerenciamento de aplicações contêinerizadas em grande escala.
